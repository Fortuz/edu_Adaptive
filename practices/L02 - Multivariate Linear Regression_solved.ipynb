{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L02%20-%20Multivariate%20Linear%20Regression_solved.ipynb)\n",
    "\n",
    "# Labor 02: Multivariate linear regression\n",
    "\n",
    "### Property prices:\n",
    "\n",
    "In this exercise, we will extend our univariate linear regression model introduced in lab L01 to the multivariate linear regression case to estimate house prices.\n",
    "\n",
    "Suppose we want to sell a house, but we want to know the real value of the house so that we do not lose money on the sale. One possible way to do this is to collect data and then estimate the real estate market price of the house by building a model based on the data. Our data will be the area of the property ($m^2$) and the number of rooms (units), and the price ($) determined at the time of sale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import the packages\n",
    "We will need:\n",
    "- NumPy for array management\n",
    "- MatPlotLib pyplot package for visualization\n",
    "- Pandas for data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data! Use the Pandas package to do this and then convert it into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/Fortuz/edu_Adaptive/main/practices/assets/Lab02/Lab2data.txt',header = None).to_numpy()   \n",
    "X = data[:,0:2]                                                # sort X\n",
    "m = X.shape[0]                                                 # number of samples\n",
    "Y = data[:,2].reshape(m,1)                                     # sort Y\n",
    "\n",
    "print('X:',X.shape)                                            # check the shapes\n",
    "print('Y:',Y.shape)\n",
    "print('Number of samples: ',m)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Normalising values (Feature scaling & Mean normalization)\n",
    "\n",
    "The features of the data points may be in different magnitudes. In our case, it is easy to see that there is at least an order of magnitude difference between the area of the property and the number of rooms. In such cases, it is worth normalising our values so that they fall within the same order of magnitude and all input variables fall within the range of [-1..1] or [0..1] interval. This operation will promote convergence, as there will be no dominant variable present to suppress the effect of other variables. <br>\n",
    "For this we will use the following relationship:\n",
    "\n",
    "$ x = \\frac{x - mean(x)}{std(x)} $\n",
    "\n",
    ", with other words the mean of the samples is subtracted from a given sample (mean normalization) and divided by the standard deviation of the samples (feature scaling).\n",
    "\n",
    "Graphically imagining around the origin makes it easier to find the line that covers our data according to our hypothesis. It is therefore useful to trasform our data into this region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab02/Pics/L02_Scaling.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the normalizing function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "######################################################    \n",
    "    avg    = np.mean(X,axis =0)                          # calculation of the average per column\n",
    "    sigma  = np.std(X,axis = 0, ddof = 1)                # calculation of the standard deviation per column (corrected empirical)\n",
    "    X_norm = (X-avg)/sigma \n",
    "######################################################\n",
    "    return X_norm, avg, sigma                            # return the formula based result \n",
    "\n",
    "print('Normalizing X vector ...')                       \n",
    "X_norm,avg,sigma = featureNormalize(X)                   # normalization\n",
    "X_norm=np.column_stack((np.ones(m),X_norm))              # add bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After normalization, the BIAS is added to the input X matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Gradient based method\n",
    "Following the example of the previous exercise, we create the gradient method in multivariable form! Our data structure is as follows.\n",
    "\n",
    "<img src=\"assets/Lab02/Pics/L02_Matrixok.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hypothesis function for the multivariate case can be written as follows:\n",
    "\n",
    "$ h_{w}(x)=w_0x_0+w_1x_1+w_2x_2+ ... +w_nx_n $ <br>\n",
    "\n",
    "And with matrix operations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab02/Pics/L02_XW.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula of the cost function: <br>\n",
    "\n",
    "$ C(W)=C(w_0,w_1,...,w_n)=\\frac{1}{2m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)^2 $\n",
    "\n",
    "Tip: When programming, take advantage of\n",
    "\n",
    "<img src=\"assets/Lab02/Pics/L02_Sum.png\" width=\"550\">\n",
    "\n",
    "General weight update formula of the gradient method:\n",
    "\n",
    "$ \\color{red}{(j=0...n)}\\hspace{7mm} w_j:=w_j-\\mu\\frac{1}{2m}\\sum_{i=1}^{m}((h_w(x^i)-y^i)\\cdot x_j^i) $\n",
    "\n",
    "$\\color{red}{Pay\\ attention\\ to\\ the\\ simultaneous\\ update!}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost function\n",
    "def computeCostMulti(X,Y,W):\n",
    "#############################################     \n",
    "    C=0\n",
    "    C=(1/(2*m))*((X@W-Y).T)@(X@W-Y) \n",
    "############################################# \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent Method\n",
    "def gradientDescentMulti(X,Y,W,lr,epochs):              \n",
    "#############################################    \n",
    "    C_history = np.zeros((epochs,1))\n",
    "    temp= np.zeros((np.size(W,0),1))\n",
    "\n",
    "    for iteration in range(0,epochs):\n",
    "       for i in range(0,np.size(W,0)):\n",
    "           temp[i] = W[i]-(lr/(2*m))*np.sum((X@W-Y)*(X[:,i].reshape(m,1)), axis=None)\n",
    "       W=temp\n",
    "       C_history[iteration]=computeCostMulti(X,Y,W)\n",
    "############################################# \n",
    "    return W, C_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Running gradient descent ...')\n",
    "lr = 0.015                                                  # learning rate\n",
    "epochs = 1200                                               # number of epochs\n",
    "W=np.zeros((3,1))                                           # initial weights (0;0;0)\n",
    "W,C_history= gradientDescentMulti(X_norm,Y,W,lr,epochs)     # Use the Gradient Descent Method\n",
    "print('''Weights expected from gradient descent (approx.):\n",
    " [[340372.05039403]\n",
    " [109434.51046856]\n",
    " [ -5454.97874429 ]]\n",
    "''')\n",
    "print('Weights computed from gradient descent:\\n', W)\n",
    "\n",
    "if int(W[0]) == 340372 and int(W[1]) == 109434 and int(W[2]) == -5454:\n",
    "    print(\"\\n The gradientDescentMulti() function is good. You can proceed.\")\n",
    "else:\n",
    "    print(\"\\n Something not right. Please modify the function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the convergence with the help of a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C_history)                                                                 # C_history plot\n",
    "plt.title(\"Gradient descent algorithm's effect through the iterations\",pad= 20)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Cost function value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Prediction\n",
    "Let's estimate the price of a 1650 $m^2$, 3 bedroom property! Watch for the normalization of the data, which is also neccessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(FEET, BED):\n",
    "#############################################    \n",
    "    price = (np.array([1, ((FEET-avg[0])/sigma[0]), (BED-avg[1])/sigma[1]]))@W\n",
    "############################################# \n",
    "    return price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEET = 1650\n",
    "BED = 3\n",
    "price = predict(FEET, BED)\n",
    "print('''Prediction for a 1650 sq-ft / 3 bedroom house:\n",
    "(predicted price should be approx. $293000) %.2f''' % price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In a slightly different way: with high level packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/Fortuz/edu_Adaptive/main/practices/assets/Lab02/Lab2data.txt',header = None)   \n",
    "\n",
    "X = data.iloc[:, 0:2].values.reshape(-1,2)                              # arrange X\n",
    "Y = data.iloc[:, 2].values.reshape(-1,1)                                # arrange Y \n",
    "\n",
    "lin_reg = LinearRegression()                                            # creation of a linear regression model class\n",
    "lin_reg.fit(X,Y)                                                        # fit based on X,Y\n",
    "\n",
    "pred = lin_reg.predict([[1650,3]])                                      # prediction for a house with 1650 m^2 and 3 bedrooms\n",
    "print('Prediction for a 1650 sq-ft / 3 bedroom house:\\n %.2f' % pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
