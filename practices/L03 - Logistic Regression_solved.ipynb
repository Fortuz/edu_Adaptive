{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L03%20-%20Logistic%20Regression_solved.ipynb)\n",
    "\n",
    "# Labor 03: Logistic Regression\n",
    "### University admission:\n",
    "\n",
    "In this exercise, we will use logistic regression to predict the admission of a given student to the university.\n",
    "\n",
    "Suppose that we are university administrators and we want to determine the probability of a given applicant to be admitted successfully based on the results of two admission tests. We have at our disposal the results so far, labelled as to whether the student's application was successful or not.\n",
    "\n",
    "We can use this dataset for our logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import the neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('https://raw.githubusercontent.com/Fortuz/edu_Adaptive/main/practices/assets/Lab03/Lab3data.txt', header = None).to_numpy()\n",
    "X = data[:,0:2]                                                    # arrange X\n",
    "m,n = X.shape                                                      # m - number of data points | n - number of features\n",
    "Y = data[:,2].reshape(m,1)                                         # arrange Y\n",
    "del data                                                           # delete unnecessary data\n",
    "X_original = X\n",
    "\n",
    "print('X:', X.shape)                                               # print some helpful values\n",
    "print('Y:', Y.shape)\n",
    "print('Number of data points:',m)\n",
    "print('Number of features:',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X,Y):\n",
    "    pos=[]                                                  # Admited - 1\n",
    "    neg=[]                                                  # Denied - 0\n",
    "\n",
    "    for i in range(0,Y.size):                               # Y is used to check whether the person has been admitted\n",
    "        if Y[i] ==0:                                        # if not then the given elements of X go to neg\n",
    "            neg.append(X[i,:])\n",
    "        elif Y[i] ==1:                                      # if yes then the given elements of X go to pos\n",
    "            pos.append(X[i,:])\n",
    "\n",
    "    neg = np.array(neg)                                   \n",
    "    pos = np.array(pos)                                     \n",
    "\n",
    "    plt.scatter(neg[:,0],neg[:,1],marker='x',c=\"r\", label=\"Not admitted\")   \n",
    "    plt.scatter(pos[:,0],pos[:,1],marker='o',c=\"g\", label=\"Admitted\")       \n",
    "    plt.title(\"Training data\")\n",
    "    plt.xlabel(\"Exam 1 score\")\n",
    "    plt.ylabel(\"Exam 2 score\")\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.show()\n",
    "\n",
    "    return pos, neg                                          \n",
    "\n",
    "pos,neg=plotData(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the two categories are quite distinct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Data preparation\n",
    "\n",
    "##### Data normalization\n",
    "If our data are of the same order of magnitude, normalisation is not strictly necessary, but can be done (see L02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalization(X):\n",
    "#######################################    \n",
    "    mean = np.mean(X,axis=0)                               \n",
    "    std = np.std(X, axis=0,ddof=1)                                                                \n",
    "\n",
    "    X_norm = (X - mean) / std                              \n",
    "#######################################\n",
    "    return X_norm,mean,std                                 \n",
    "\n",
    "X,mean,std=featureNormalization(X)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add bias to the X matrix\n",
    "\n",
    "As we have done in the previous steps, we add a column of ones to the X matrix to preserve the axis intercept in our hypothesis (See L01 - L02)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "X=np.column_stack((np.ones((m,1)),X))       # bias\n",
    "#######################################\n",
    "print(X.shape)                              # check the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Model\n",
    "\n",
    "Since we are facing a classification problem, our linear regression model will not be fully adequate. Let's look at a simple example. We are trying to decide whether a tumour is lethal or not based on tumour size.\n",
    "\n",
    "<img src=\"assets/Lab03/Pics/L03_Tumor.png\" width=\"400\">\n",
    "\n",
    "If the method introduced so far is applied and fit a line using the following hypothesis.\n",
    "\n",
    "$ h_w(x)=XW $ \n",
    "\n",
    "However, our line exceeds the range of [0,1] and is not precise enough. By then defining a limit for our fitted line, let it be 0.5, we can decide what our prediction will be.\n",
    "\n",
    "If $h_w(x)\\geq 0.5$, then \"y=1\", so it is lethal. <br>\n",
    "If $h_w(x)< 0.5$, then \"y=0\", so it is not lethal.  <br>\n",
    "\n",
    "However, our prediction may fall below 0 or above 1, which is unnecessary. It would be better to find a bounded hypothesis function that satisfies the following criterion.\n",
    "\n",
    "$0\\leq h_w(x) \\leq 1$\n",
    "\n",
    "Let us introduce the sigmoid function, which satisfies this criterion of being bounded on [0,1].\n",
    "\n",
    "Sigmoid:\n",
    "\n",
    "$ g(z) = \\frac{1}{1+e^{-z}} $\n",
    "\n",
    "<img src=\"assets/Lab03/Pics/L03_sigmoid.png\" width=\"450\">\n",
    "\n",
    "Using the sigmoid function, we effectively assign a probability to each sample of the chance that the tumour size is lethal.\n",
    "\n",
    "$h_w(x)=P(y=1|X, W)$\n",
    "\n",
    "And we can extend our hypothesis as follows: <br>\n",
    "\n",
    "$ h_w(x) = g(XW) $ <br>\n",
    ", where $ g(XW) = \\frac{1}{1+e^{-XW}} $\n",
    "\n",
    "$g(XW)\\geq0.5$ <br>\n",
    "if $WX\\geq0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at two simple examples of classification using a sigmoid function. <br>\n",
    "\n",
    "#### Linear case:\n",
    "\n",
    "$ h_w(x)=g(w_01+w_1x_1+w_2x_2) $\n",
    "\n",
    "$w=[-3\\ 1\\ 1]$ \n",
    "\n",
    "Prediction: $y=1$ ha $-3+x_1+x_2\\geq0$\n",
    "\n",
    "$x_1+x_2\\geq3$ \n",
    "\n",
    "<img src=\"assets/Lab03/Pics/L03_pelda_1.png\" width=\"200\">\n",
    "\n",
    "#### Non-linear case:\n",
    "\n",
    "$ h_w(x)=g(w_01+w_1x_1+w_2x_2+w_3x_1^2+w_4x_2^2) $ \n",
    "\n",
    "$w=[-1\\ 0\\ 0\\ 1\\ 1]$ \n",
    "\n",
    "Prediction: $y=1$ ha $-1+x_1^2+x_2^2\\geq0$\n",
    "\n",
    "$x_1^2+x_2^2\\geq1$\n",
    "\n",
    "<img src=\"assets/Lab03/Pics/L03_pelda_2.png\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a sigmoid function then test it for the following values: -6, 0, 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "############################################    \n",
    "    g = 1/(1 + np.exp(-z))\n",
    "############################################    \n",
    "    return g                               \n",
    "\n",
    "print('Function return for value -6 : %.3f' % sigmoid(-6))           # test for -6\n",
    "print('Function return for value  0 : %.3f' % sigmoid(0))            # test for 0 \n",
    "print('Function return for value  6 : %.3f' % sigmoid(6))            # test for 6  \n",
    "\n",
    "if sigmoid(-6) < 0.01 and sigmoid(0) == 0.5 and sigmoid(6) > 0.99:\n",
    "    print(\"\\n The sigmoid() function is good.\")\n",
    "else:\n",
    "    print(\"\\n Something wrong. Correct the sigmoid function!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 Cost function\n",
    "Since we have changed our hypothesis function, we need to change the cost function we have been using. To adapt it to the task. The MSE introduced in our previous examples provided a reasonably smooth convergence, with our modified hypothesis it is not the best choice for solving classification problems as it will result in a non-convex function with many local mininmum points. The question is, can we find a cost function that can define a convex cost function in classification problems? On a convex function, our gradient method is much less likely to get stuck at a local minimum point.\n",
    "\n",
    "<img src=\"assets/Lab03/Pics/L03_Costfunction.png\" width=\"600\">\n",
    "\n",
    "Let:\n",
    "\n",
    "$ C(w) = {-log(h_w(x)),\\   ha\\ y=1} $ \n",
    "\n",
    "$ C(w) = {-log(1-h_w(x)),\\ ha\\ y=0} $\n",
    "\n",
    "<img src=\"assets/Lab03/Pics/L03_LogCost.png\" width=\"300\">\n",
    "\n",
    "So we use the following relation as the cost function:\n",
    "\n",
    "$ C(w) = \\frac{1}{m} \\sum{-y^i\\cdot\\log(h_w(x^i))-(1-y^i)\\cdot\\log(1-h_w(x^i))} $\n",
    "\n",
    "\n",
    "### 7: Gradient Descent Algorithm\n",
    "Our aim remains to minimise the cost function. Descending the gradient is still a workable way of truncating the weights using the derivative of the cost function.\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w)$ \n",
    "\n",
    "The partial derivative of the cost function can be calculated in the usual way:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j}C(w)=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_j^i $\n",
    "\n",
    "\n",
    "### Define the costFunction\n",
    "\n",
    "Write the definition of costFunction and test it with two values of w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunction(w,X,Y):\n",
    "######################################################################\n",
    "    h = np.array(sigmoid(X@w))                                              # prediction\n",
    "    C = (1/m) * np.sum(((-Y) * np.log(h)) - ((1-Y)*(np.log(1-h))))          # cost\n",
    "\n",
    "    grad = (X.transpose())@(h-Y)/m                                          # gradient\n",
    "######################################################################\n",
    "    return C, grad                                                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_w = np.zeros(((n+1),1))                                             # initial weights (null vector)\n",
    "C1,grad1 = costFunction(initial_w,X,Y)                                      # test the cost function\n",
    "\n",
    "print('''Cost and Gradient at initial weights (zeros):\n",
    "Expected cost (approx.): 0.693\n",
    "Computed:''',C1)\n",
    "print('''Expected gradient(approx.):\n",
    " [[-0.1]\n",
    " [-0.28]\n",
    " [-0.25]]\n",
    "Computed:\\n''',grad1)\n",
    "\n",
    "test_w = np.array([[-24], [13], [16]])                                    # test weights [-24;0.2;0.2]\n",
    "C2, grad2 = costFunction(test_w,X,Y)                                      # test the cost function\n",
    "print('\\nTest weights:',test_w.transpose())\n",
    "print('''Cost and Gradient  at test weights:\n",
    "Expected cost (approx.): 7.74\n",
    "Computed:''',C2)\n",
    "print('''Expected gradient(approx.):\n",
    " [[-0.44]\n",
    " [-0.14]\n",
    " [-0.06]]\n",
    "Computed:\\n''',grad2)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Definition of the Gradient Descent Method\n",
    "\n",
    "We run the algorithm up to a certain number of iterations in the following test for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,w,learning_rate,num_iters):\n",
    "    C_history = []                                      \n",
    "##################################################\n",
    "    for i in range(num_iters):                          \n",
    "        C, grad = costFunction(w,X,Y)                   \n",
    "        w = w - (learning_rate*grad)                    \n",
    "        C_history.append(C)                             \n",
    "##################################################\n",
    "    return w, np.array(C_history)                       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Trying out the gradient descent method for multiple learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([[0],[0],[0]])     # initial weights\n",
    "epoch = 400\n",
    "\n",
    "w_a, C_history_a = gradientDescent(X,Y,w,0.01,epoch)                             \n",
    "plt.plot(range(C_history_a.size), C_history_a, label= \"learning r.:0.01\")\n",
    "                                                       \n",
    "w_b, C_history_b = gradientDescent(X,Y,w,0.1, epoch)                             \n",
    "plt.plot(range(C_history_b.size), C_history_b, label= \"learning r.:0.1\")\n",
    "\n",
    "w, C_history = gradientDescent(X,Y,w,1,epoch)\n",
    "plt.plot(range(C_history.size), C_history, label= \"learning r.:1\")              \n",
    "\n",
    "plt.title(\"Effect of the different constants on the cost function\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost function value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows how the learning rate affected our results:<br>\n",
    "When the learning rate $(\\mu)$ is chosen to be small, convergence is slow. <br>\n",
    "As the learning rate $(\\mu)$ increases, convergence accelerates.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''The cost function at found weights by the gradient descent alg.:\n",
    "Expected (approx): 0.203\n",
    "Computed: %.04f''' % C_history[-1])\n",
    "print('''Weights expected (approx.):\n",
    "[1.658 3.883 3.619]\n",
    "Weights computed: \\n''', w.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Visualization of the decision limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pos[:,0],pos[:,1],c=\"g\", marker=\"o\",label=\"Admitted\")     \n",
    "plt.scatter(neg[:,0],neg[:,1],c=\"r\",marker=\"x\",label=\"Not admitted\")  \n",
    "\n",
    "Exam1_val     = np.array([min(X_original[:, 0])-2, max(X_original[:, 0]+2)])  # add two x values to the decision limit\n",
    "Exam1_norm = (Exam1_val - mean[0]) / std[0]\n",
    "Exam2_norm = (-w[0]-w[1]*Exam1_norm)/w[2]                                     # calculate the corresponding y\n",
    "Exam2_val     = (Exam2_norm * std[1]) + mean[1]\n",
    "\n",
    "plt.plot(Exam1_val,Exam2_val,\"k\")                                             # draw the decision boundary in black\n",
    "plt.title(\"Decision boundary and the training data\")        \n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10: Prediction\n",
    "When calculating the prediction, we have to make sure that we perform the same operations on the sample as we did in the data preparation phase before the training. So if we have normalised our data, we must also normalise the new data and add the BIAS. Then we can use our saved weight vector to calculate our prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X):              \n",
    "###########################################    \n",
    "    X = (X-mean)/std                             # normalization\n",
    "    X = np.append(np.ones((1)),X)                # bias\n",
    "    h = sigmoid(X@w)                             \n",
    "    if h >= 0.5:\n",
    "        p = 1\n",
    "    else:\n",
    "        p = 0         \n",
    "###########################################    \n",
    "    return p, h                                    \n",
    "\n",
    "NewScore = np.array([45,85])\n",
    "pred, h =predict(NewScore)           \n",
    "print('''Expected result of the prediction with [45 , 85] (approx.):\n",
    "Accepted (1) with 0.767 possibility\n",
    "Predicted: %.0f with %.4f possibility''' % (pred, h[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11: Accuracy\n",
    "Let's examine how our linear model performs when evaluating the original data. Calculate the accuracy of the algorithm. To do this, we can use the predict() function written earlier or we can evaluate all the samples in a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateAccuracy():                                    \n",
    "#######################################\n",
    "    predictions= (sigmoid(X@w)>0.5)                         # calculate the prediction for each original X\n",
    "    accuracy=(sum(predictions==Y)/m)*100                    # if greater than 0.5 then pos otherwise neg\n",
    "#######################################\n",
    "    return accuracy                                         # compare the result with the elements of Y \n",
    "                                                            # and calculate a percentage to reflect the accuracy\n",
    "\n",
    "print(float(calculateAccuracy()), '% accuracy (approx. 89.0 % expected)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher level implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression                       \n",
    "\n",
    "data = pd.read_csv('Lab3data.txt', header = None)       # Load data\n",
    "XX = data.iloc[:, 0:2].values.reshape(-1, 2)            # Separation of data\n",
    "YY = data.iloc[:, 2].values.reshape(-1,)                # LogReg fits to a 1d array\n",
    "\n",
    "logReg = LogisticRegression().fit(XX,YY)\n",
    "test = np.array([[45, 85]])                             \n",
    "pred = logReg.predict(test)                             # prediction\n",
    "pred_p = logReg.predict_proba(test)                     # probability of the prediction\n",
    "\n",
    "print(\"\"\"Prediction for the approval:\"\"\",int(pred),\"\"\"\n",
    "The value of the probability:\"\"\",pred_p[0,1])\n",
    "\n",
    "acc = logReg.score(XX,YY)                               # calculate accuracy\n",
    "print('Accuracy on the training data:',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
