{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labor 09 - Support Vektor Machine (SVM)\n",
    "\n",
    "Klasszifikációs problémák esetén a Support Vector Machine (SMV) egy jó megoldás lehet. Az SVM segítségével lineárisan szeparálható adathalmaz esetén megtalálhatjuk azt adatokat a lehető legszélesebb határral elválasztó egyenest.\n",
    "\n",
    "Két lineárisan szeparálható claster esetén végtelen sok lineáris határ létezik. Ezek közül szeretnénk azt kiválasztani, amelyik mindkét claster pontjaitól a legtávolabb húzódik."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab09/Pics/L09_SVMexample.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A két osztályunk kékkel és pirossal jelölve. A folytonos fekete vonal a döntési határunk. A kiemelt pontok képezik a támogató vektorokat (support vector). A távolság a kiemelt pontok és az elválasztó határ között az összes lehetséges variáció közül ebben az esetben maximális. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A L03 laborhoz visszatérve az nagyából annyit jelent, hogy a sigmoid függvényünk bevezetésénél nem csupán egy éles elválasztóhatárt szeretnénk definiálni, hanem egy egész sávot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ h_w(x) = \\frac{1}{1+e^{-XW}} $ <br>\n",
    "\n",
    "$ h_w(x) = g(XW)$\n",
    "\n",
    "$ h_w(x) = g(z)$\n",
    "\n",
    "Ha $y=1$ azt akarjuk, hogy $z\\geq1$ legyen.<br>\n",
    "Ha $y=1$ azt akarjuk, hogy $z\\leq-1$ legyen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab09/Pics/L09_SVMsigmoid.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A költség függvényt az alábbi formában definiáltuk:\n",
    "\n",
    "$ C(w) = \\frac{1}{m} \\sum{-y^i\\cdot\\log(h_w(x^i))-(1-y^i)\\cdot\\log(1-h_w(x^i))} $\n",
    "\n",
    ", ahol <br>\n",
    "\n",
    "$ h_w(x) = \\frac{1}{1+e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab09/Pics/L09_newcost.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vezessünk be két új függvényt: $cost_{1}(z)$ és $cost_{0}(z)$ <br>\n",
    "vagyis a kitűzött határok 1 és -1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A költség függvény minimalizálása továbbra is a feladatunk, amely az alábbiak szerint módosult:\n",
    "\n",
    "$min_w\\ \\frac{1}{m} \\Big[ \\sum_{i=1}^m y^{(i)}\\cdot \\big(-log(h_w(x^{(i)})+(1-y^{(i)})\\big)\\cdot \\big(-log(1-h_w(x^{(i)}))\\big)\\Big] +\\frac{\\lambda}{2m}\\sum_{j=1}^nw_j^2 $\n",
    "\n",
    "$ min_w\\ {\\color{red} C} \\Big[ \\sum_{i=1}^m y^{(i)}\\cdot \\color{red} {cost_1}(h_w(x^{(i)})+(1-y^{(i)})\\big)\\cdot\\color{red} {cost_0}(1-h_w(x^{(i)}))\\big)\\Big] +\\frac{1}{2}\\sum_{j=1}^nw_j^2$\n",
    "\n",
    ", ahol $C = 1/\\lambda$ <br>\n",
    "<font color='red'>[FIGYELEM! A képleten belüli C nem azonos a költség függvény jelölésére használt $C(w)$ -vel]. A jelölést a python kódban használt szintaktika miatt használjuk. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A korábban az L03-L04 laborokban látott bevezetés alapján ezen megfontolássok segítségével tehát az SVM módszer kiterjeszthető a nem lineárisan szeparálható osztályozási feladatokra is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab09/Pics/L09_SVMtransfer.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elméleti összefoglalás\n",
    "\n",
    "A kölönböző klasszifikációs problémák legbiztonségosabb határának megtalálásához egy jó módszer az SVM. Alap koncepcióját tekintve lineárisan szeparálható feladatok megoldására alkalmas, viszont különböző kernelek segítségével gyakorlatilag tetszőleges feladatok megoldására alkalmassá tehető.\n",
    "\n",
    "A kernel trükk lényege, hogy gyakorlatilag az adatainkat egy magasabb dimenzióba transformáljuk, ahol már lineárian szeparálhatóak leszenk.\n",
    "\n",
    "Szemléltető példának az alábbi 2D adathalmaz lineárisan nem szeparálható.\n",
    "\n",
    "<img src=\"assets/Lab09/Pics/L09_NonSep.png\" width=\"350\">\n",
    "\n",
    "Gaussian (vagy a későbbiekben Radial basis function kernelt használva)\n",
    "\n",
    "<img src=\"assets/Lab09/Pics/L09_Gauss.png\" width=\"350\">\n",
    "\n",
    "A Gauss kernel matematikailag egy végtelen dimenziójú térbe transzformálja az adatainkat. Ennek segítségével a megoldásunkat egy magasabb dimenziójú térből nyerhetjük.\n",
    "\n",
    "Az adataink 3D reprezentációja már lineárisan szeparálható adathalmaz lesz.\n",
    "\n",
    "<img src=\"assets/Lab09/Pics/L09_Sep.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az elméleti összefoglaló a matematikai háttér felületes áttekintése volt csupán, hogy kapjunk egy átfogó képet arról, mik is működnek a következő kódban alkalmazott magasabb szintű Python csomagokat használó kódunkban."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Importálás / adatok betöltése"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = loadmat('Lab9data1.mat')\n",
    "X1 = data[\"X\"]\n",
    "Y1 = data[\"y\"]\n",
    "\n",
    "print(X1.shape)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualizálás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSeparation(X,Y):\n",
    "    CLR1 = []                                    # cluster1\n",
    "    CLR2 = []                                    # cluster2\n",
    "    \n",
    "    for i in range(Y.shape[0]):\n",
    "        if (Y[i] == 0):\n",
    "            CLR1.append(X[i,:])\n",
    "        elif (Y[i] == 1):\n",
    "            CLR2.append(X[i,:])\n",
    "            \n",
    "    CLR1 = np.asmatrix(CLR1)\n",
    "    CLR2 = np.asmatrix(CLR2)\n",
    "    return CLR1, CLR2\n",
    "\n",
    "CLR11,CLR12 = dataSeparation(X1,Y1)\n",
    "\n",
    "plt.plot(CLR11[:,0],CLR11[:,1],'gx')\n",
    "plt.plot(CLR12[:,0],CLR12[:,1],'ro')\n",
    "plt.title(\"Első adathalmaz\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Az adatainkat megfigyelve láthatjuk, hogy ez az adathalmaz lineárisan szeparálható. Használjuk az SVM modellt, melyhez minden szükséges algoritmust az sklearn csomag tartalmaz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Lineáris SVM tanítása"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ehhez a feladathoz nem lesz szükségünk semmi extra kernelre (nem kell trükközni), így szimplán beállíthatjuk a kernel paraméternek a lineáris opciót.\n",
    "A C paraméternek kezdetnek a 0.5 értéket választjuk. (a ravel() függvényről később beszélünk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C=0.5,kernel=\"linear\")\n",
    "print(classifier)\n",
    "classifier.fit(X1,Y1.ravel())\n",
    "\n",
    "plt.plot(CLR11[:,0],CLR11[:,1],'gx')\n",
    "plt.plot(CLR12[:,0],CLR12[:,1],'ro')\n",
    "\n",
    "# plotting the decision boundary\n",
    "X_1,X_2 = np.meshgrid(np.linspace(X1[:,0].min(),X1[:,1].max(),num=100),np.linspace(X1[:,1].min(),X1[:,1].max(),num=100))\n",
    "desBoundary1 = classifier.predict(np.array([X_1.ravel(),X_2.ravel()]).T).reshape(X_1.shape)\n",
    "plt.contour(X_1,X_2,desBoundary1,1,colors=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Láthatjuk, hogy 1 pont besorolása nem a megfelelő csoportba történt. Ez a pont ránézésre mérési hiba is lehet. Ha szeretnénk, hogy a megoldásunk ezt a kiugró pontot is megfelelően lekezelje hangoljuk a C paramétert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tesztelés: C = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = SVC(C=100,kernel=\"linear\")\n",
    "print(classifier2)\n",
    "classifier2.fit(X1,Y1.ravel())\n",
    "\n",
    "plt.plot(CLR11[:,0],CLR11[:,1],'gx')\n",
    "plt.plot(CLR12[:,0],CLR12[:,1],'ro')\n",
    "\n",
    "# plotting the decision boundary\n",
    "X_1,X_2 = np.meshgrid(np.linspace(X1[:,0].min(),X1[:,1].max(),num=100),np.linspace(X1[:,1].min(),X1[:,1].max(),num=100))\n",
    "desBoundary2 = classifier2.predict(np.array([X_1.ravel(),X_2.ravel()]).T).reshape(X_1.shape)\n",
    "plt.contour(X_1,X_2,desBoundary2,colors=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Láthatjuk, hogy a feladat így is megoldható, viszont a biztonsági sáv a két klaszter között jelentősen lecsökkent. Könnyen lehet, hogy túltanulás esete áll fenn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Názzük meg, hogyan lehet alkalmazni az SVM módszerét egy komplexebb adathalmaza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Második adathalmaz betöltése és vizualizálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('Lab9data2.mat')\n",
    "X2 = data[\"X\"]\n",
    "Y2 = data[\"y\"]\n",
    "print(X2.shape)\n",
    "print(Y2.shape)\n",
    "\n",
    "CLR21,CLR22 = dataSeparation(X2,Y2)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(CLR21[:,0],CLR21[:,1],'gx')\n",
    "plt.plot(CLR22[:,0],CLR22[:,1],'ro')\n",
    "plt.title(\"Második adathalmaz\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Látható, hogy ez az adathalmaz nem lineárisan szeparálható. Kernelezésre lesz szükség."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Gauss kernel\n",
    "\n",
    "Az sklearn csomag SVC függvényének kernel paraméterének az RBF kulcsszót megadva válaszhatjuk ki a Gauss gernelt (RBF = Radial basis function)\n",
    "\n",
    "A ravel() függvénynek itt lesz szerpe, mivel a magasabb dimenzióba transzformálás során figyelni kell arra is, hogy az Y is a megfelelő dimenzióra bővüljön."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SVC(kernel = \"rbf\", gamma=30)\n",
    "print(classifier3)\n",
    "classifier3.fit(X2,Y2.ravel())\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(CLR21[:,0],CLR21[:,1],'gx')\n",
    "plt.plot(CLR22[:,0],CLR22[:,1],'ro')\n",
    "plt.title('Második adathalmaz és a döntési határ')\n",
    "\n",
    "X_5,X_6 = np.meshgrid(np.linspace(X2[:,0].min(),X2[:,1].max(),num=100),np.linspace(X2[:,1].min(),X2[:,1].max(),num=100))\n",
    "desBoundary3 = classifier3.predict(np.array([X_5.ravel(),X_6.ravel()]).T).reshape(X_5.shape)\n",
    "\n",
    "plt.contour(X_5,X_6,desBoundary3,colors = \"b\")\n",
    "plt.xlim(0,1.05)\n",
    "plt.ylim(0.35,1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Harmadik adathalmaz vizualizálása"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"Lab9data3.mat\")\n",
    "X3 = data[\"X\"]\n",
    "Y3 = data[\"y\"]\n",
    "Xval = data[\"Xval\"]\n",
    "Yval = data[\"yval\"]\n",
    "print(X3.shape)\n",
    "print(Y3.shape)\n",
    "\n",
    "CLR31,CLR32 = dataSeparation(X3,Y3)\n",
    "\n",
    "plt.plot(CLR31[:,0],CLR31[:,1],\"go\")\n",
    "plt.plot(CLR32[:,0],CLR32[:,1],\"rx\")\n",
    "plt.title(\"Harmadik adathalmaz\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ezen az adathalmazon a Paraméter hangolást fogjuk bemutatni néhány előr meghatározott C konstans értékkel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Paraméter becslése néhány C értékével"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset3Params(X, y, Xval, yval,vals):\n",
    "    acc = 0\n",
    "    best_c=0\n",
    "    best_gamma=0\n",
    "    for i in vals:\n",
    "        C= i\n",
    "        for j in vals:\n",
    "            gamma = 1/j\n",
    "            classifier = SVC(C=C,gamma=gamma)\n",
    "            classifier.fit(X,y)\n",
    "            prediction = classifier.predict(Xval)\n",
    "            score = classifier.score(Xval,yval)\n",
    "            if score>acc:\n",
    "                acc =score\n",
    "                best_c =C\n",
    "                best_gamma=gamma\n",
    "    return best_c, best_gamma\n",
    "\n",
    "C_vals = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n",
    "C, gamma = dataset3Params(X3, Y3.ravel(), Xval, Yval.ravel(),C_vals)\n",
    "classifier4 = SVC(C=C,gamma=gamma)\n",
    "print(classifier4)\n",
    "classifier4.fit(X3,Y3.ravel())\n",
    "print('Best values:\\nC =',C,'\\ngamma =',gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(CLR31[:,0],CLR31[:,1],\"go\")\n",
    "plt.plot(CLR32[:,0],CLR32[:,1],\"rx\")\n",
    "plt.title(\"Harmadik adathalmaz és a döntési határ\")\n",
    "\n",
    "# plotting the decision boundary\n",
    "X_7,X_8 = np.meshgrid(np.linspace(X3[:,0].min(),X3[:,1].max(),num=100),np.linspace(X3[:,1].min(),X3[:,1].max(),num=100))\n",
    "desBoundary4 = classifier4.predict(np.array([X_7.ravel(),X_8.ravel()]).T).reshape(X_7.shape)\n",
    "plt.contour(X_7,X_8,desBoundary4,colors=\"b\")\n",
    "plt.xlim(-0.6,0.3)\n",
    "plt.ylim(-0.7,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">Ez a laborgyakorlat felhasznál elemeket Andrew Ng Machine Learning c. kurzusából.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
