{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L11%20-%20K-Means_solved.ipynb)\n",
    "\n",
    "# Labor 11 - K-Means clastering\n",
    "\n",
    "\n",
    "### 0: Background\n",
    "The main groups of machine learning algorithms are illustrated in the figure below, one of the main groups being unsupervised learning algorithms.\n",
    "\n",
    "<img src=\"assets/Lab11/Pics/L11_groups.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Korábbi laborjaink során részletesen megismerkedtünk a felügyelt tanulással (Supervised Learning) és ennek eseteivel, osztályozás és regresszió. Ebben a gyakorlatban a felügyelet nélküli tanulás (Unsupervised Learning) és azon belül is a klaszterezés lesz a téma. \n",
    "\n",
    "Mikor lehet szükség felügyelet nélküli tanításra?\n",
    "Olyan esetekben például, amikor nem ismerjük a kimenetet, de valamiféle mintát keresünk az adatunkban. Piaci résztvevők elemzése, felbontása vagy kapcsolati háló elemzés. \n",
    "\n",
    "<img src=\"assets/Lab11/Pics/L11_example.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim may be to identify small cohesive groups, also known as clusters. One possible solution is to use the K-means algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import packages\n",
    "\n",
    "The initial data will be 2D point cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Data load\n",
    "\n",
    "First tha data will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab11/Lab11data.mat\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab11/bird_small.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the data is donloaded to the local workspace it can be loaded into the notebbok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"Lab11data.mat\")\n",
    "X = data[\"X\"]\n",
    "plt.plot(X[:,0],X[:,1], 'yo')\n",
    "print('X shape:',X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first we would say that the data can be classified into 3 clusters according to the structure that can be discovered in the data set. In terms of how the K-Means algorithm works, we need to determine the number of possible clusters. So let us make this 3. \n",
    "\n",
    "The initial cluster midpoints can be defined either explicitly or randomly.\n",
    "Furthermore, they can be points chosen from the sample or even starting points not included in the sample set. \n",
    "\n",
    "What should be observed is that the number K (as number of clusters should be less than the sample number)\n",
    "\n",
    "In our palette we have predefined the coordinates of the 3 starting cluster centres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3                                                           # Number of clusters\n",
    "init_centroids = np.array([[3,3],[6,2],[8,5]])                  # Initial cluster middle points\n",
    "print('Initial cluster centroids:\\n',init_centroids)\n",
    "\n",
    "plt.plot(X[:,0],X[:,1], 'yo')\n",
    "plt.plot(init_centroids[:,0],init_centroids[:,1], 'o')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to decide for each point in the sample set which cluster it belongs to, i.e. which cluster centre it is closest to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Find the closest cluster centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the following function that assigns to each point the index of the nearest cluster centre. \n",
    "\n",
    "The distance between two points can be calculated using the Pythagorean theorem. <br>\n",
    "Let the two points be $P_1(x_1, y_1)$ és $P_2(x_2, y_2)$. \n",
    "\n",
    "The distance can be calculated as follows: <br>\n",
    "\n",
    "$d = \\sqrt{(x_1-x_2)^2+(y_1-y_2)^2}$ <br>\n",
    "\n",
    "For the purposes of comparison, the root can be omitted and $d^2$ can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findClosestCentroids(X,centroids):    \n",
    "    K = centroids.shape[0]\n",
    "    idx = np.zeros((X.shape[0],1))\n",
    "    ################### CODE HERE ######################## \n",
    "    # Find the closest centroid for each data point.\n",
    "     \n",
    "    for i in range(X.shape[0]):\n",
    "        distance = np.Inf\n",
    "        for k in range(K):\n",
    "            temp2 = X[i,:]-centroids[k,:]\n",
    "            dis_t = temp2 @ temp2.T\n",
    "            if (dis_t<distance):\n",
    "                distance = (temp2 @ temp2.T)\n",
    "                temp1 = k\n",
    "        idx[i] = temp1\n",
    "\n",
    "    #####################################################    \n",
    "    return idx\n",
    "\n",
    "idx = findClosestCentroids(X,init_centroids)\n",
    "print('Closest centroids for the first 3 examples (expected: 0-2-1):\\n',idx[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Centroid calculation\n",
    "\n",
    "Once we have classified all points into a cluster, we need to review the new cluster centres. We need to recalculate the centroid of the clusters taking into account the points that were clustered. This will step the cluster midpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCentroids(X,idx,K):\n",
    "    \n",
    "    m,n = X.shape\n",
    "    centroids = np.zeros((K,n))\n",
    "    cent = []\n",
    "\n",
    "    ################### CODE HERE ########################\n",
    "    # Calculate the new cetroinds.\n",
    "        \n",
    "    for k in range(K):\n",
    "        cent = np.zeros((1,n))\n",
    "        Ck = 0\n",
    "        for i in range(m):\n",
    "            if idx[i] == k:\n",
    "                cent = cent + X[i,:]\n",
    "                Ck = Ck + 1\n",
    "        centroids[k,:] = cent * (1 / Ck)\n",
    "\n",
    "    #####################################################    \n",
    "    return centroids\n",
    "\n",
    "centroids = computeCentroids(X,idx,K)\n",
    "print(\"\"\"Centroids computed after initial finding of closest centroids:\n",
    "(expected values):\n",
    "[[2.42830111 3.15792418]\n",
    " [5.81350331 2.63365645]\n",
    " [7.11938687 3.6166844 ]]\n",
    " Computed:\n",
    " \"\"\", centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: K-Means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of the K-means algorithm (clustering of points, stepping of centres) are then run until a stopping condition is reached. This condition can be that the midpoints no longer change, or the change stays within a specified small interval, or the algorithm is run up to a prescribed iteration. In this example, we have chosen the latter solution and will run the algorithm for 5 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotkMeans(idx,num_CL,C_H,it):\n",
    "    colors = ('b','g','r','c','m','y','k')\n",
    "    \n",
    "    plt.figure\n",
    "    for i in range(num_CL):\n",
    "        CL_i = X[np.where(idx == i)[0],:]\n",
    "        plt.plot(CL_i[:,0],CL_i[:,1], colors[i]+'o',)\n",
    "        plt.plot(C_H[i,0,0:it+2],C_H[i,1,0:it+2],'kx-')\n",
    "\n",
    "    plt.show()    \n",
    "    return 0\n",
    "\n",
    "def runkMeans(X,init_cents,max_iters,plotProgress= False):\n",
    "    \n",
    "    m, n = X.shape\n",
    "    k = init_cents.shape[0]\n",
    "    idx = np.zeros((m))\n",
    "    centroids = init_cents\n",
    "    cent_hist = np.zeros((init_cents.shape[0],init_cents.shape[1],max_iters+1))\n",
    "    cent_hist[:,:,0] = init_cents\n",
    "    \n",
    "    # K-Means:\n",
    "    for i in range(max_iters):\n",
    "\n",
    "        print('Running the {} iteration of {}'.format(i+1,max_iters))\n",
    "        idx = findClosestCentroids(X, centroids)\n",
    "        centroids = computeCentroids(X, idx, k)\n",
    "        cent_hist[:,:,i+1] = centroids\n",
    "        if plotProgress:\n",
    "            plotkMeans(idx,init_cents.shape[0],cent_hist,i)\n",
    "    \n",
    "    return centroids, idx\n",
    "\n",
    "K = 5\n",
    "max_iters = 5\n",
    "init_cents = np.array([[3,3],[6,2],[8,5]])\n",
    "\n",
    "centroids, idx = runkMeans(X,init_cents,max_iters,True)\n",
    "print(\"\"\"Centroids computed by the algorithm:\n",
    "Expected:\n",
    "[[1.95399466 5.02557006]\n",
    " [3.12663743 1.1121712 ]\n",
    " [6.12919526 3.01606258]]\n",
    "Computed:\n",
    "\"\"\",centroids)\n",
    "\n",
    "print('Centroids movements')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how our initial cluster centres migrated to their final location over the 5 iterations, and how the sample classification of the dataset changed accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Problems\n",
    "\n",
    "Depending on the choice of cluster centers (especially in the case of random initialization), the algorithm will arrive at different optimal positions. Furthermore, it can easily get stuck in a local optimum if poorly distributed cluster centres are chosen.\n",
    "\n",
    "<img src=\"assets/Lab11/Pics/L11_Localopt.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important question is how to choose the value of K (number of clusters). <br>\n",
    "This is mostly done intuitively. In well-separable cases, a so-called elbow rule can be used to compare the results of several runs. This determines the point beyond which, by choosing a larger number of clusters, there is no longer a significant decrease in the cost function (here the cost function is the sum of the distances between the points and the cluster centres).\n",
    "\n",
    "<img src=\"assets/Lab11/Pics/L11_Elbow.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, there are cases where the elbow rule does not apply. For example, when we want to separate a data set with an approximately linear distribution. In this case, again, it is up to us to decide at what level of detail we want to partition the scale.\n",
    "\n",
    "<img src=\"assets/Lab11/Pics/L11_Size.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: K-Means clustering on pixels\n",
    "\n",
    "Finally, an extra example of using K-means. In the following example, the colours of an image are selected and compressed using the K-Means algorithm. The example is entirely a toy example, in reality there are much better algorithms for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kMeansInitCentroids(X,K):\n",
    "    cents = np.zeros((K,X.shape[1]))\n",
    "    randindx = np.random.permutation(X.shape[0])\n",
    "    cents = X[randindx[0:K],:]\n",
    "    \n",
    "    return cents\n",
    "\n",
    "\n",
    "image = loadmat(\"bird_small.mat\")\n",
    "A_o = image['A']\n",
    "A = A_o / 255.\n",
    "img_size = A.shape \n",
    "print(img_size)\n",
    "plt.imshow(A)\n",
    "\n",
    "X2 = A.reshape((int(A.size/3),3))\n",
    "\n",
    "K =2\n",
    "max_iters = 5\n",
    "init_cents = kMeansInitCentroids(X2,K)\n",
    "print('Initial Centroids:\\n',init_cents)\n",
    "centroids, idx = runkMeans(X2,init_cents,max_iters)\n",
    "print('K-Means clustering done...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9: Image compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = findClosestCentroids(X2,centroids)\n",
    "\n",
    "X_recovered = centroids[idx.astype(int),:]\n",
    "X_recovered = np.reshape(X_recovered, (A.shape[0], A.shape[1], A.shape[2]))\n",
    "plt.imshow(X_recovered)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
