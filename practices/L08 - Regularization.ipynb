{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L08%20-%20Regularization.ipynb)\n",
    "\n",
    "# Labor 08 - Regularisation of Linear Regression and Bias - Variance\n",
    "\n",
    "### Water flow\n",
    "\n",
    "In the first part of the exercise, a linear regression is implemented to predict the amount of water spilled from a tank based on how much water is in the tank. In the second half of the exercise, we observe the debugging of the learning algorithms and the bias and variance type errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import, load and visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import scipy.optimize as op\n",
    "\n",
    "# keras imports for the dataset and building our neural network\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras import regularizers\n",
    "from keras import optimizers\n",
    "\n",
    "\n",
    "from numpy.random import seed\n",
    "# Since this nootebook contains a random initialization it is advised to fix the seed, to get always the same random results\n",
    "seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be loaded from a publicly available file. An alternative solution would be to upload the data file directly to the google colab file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab08/Lab8data.mat\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab08/w_final.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data! Use the Pandas package to do this and then convert it into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"Lab8data.mat\")                         \n",
    "X_train = data[\"X\"]                                    \n",
    "Y_train = data[\"y\"]\n",
    "X_val   = data[\"Xval\"]\n",
    "Y_val   = data[\"yval\"]\n",
    "X_test  = data[\"Xtest\"]\n",
    "Y_test  = data[\"ytest\"]\n",
    "\n",
    "del data\n",
    "m,n = X_train.shape\n",
    "print('Shape of X:', X_train.shape)\n",
    "print('Shape of Y:', Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Visualization\n",
    "\n",
    "Let's vizualise the data set to understand it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train,Y_train,'x') \n",
    "plt.title('Training data')\n",
    "plt.xlabel('Change in water level (X)')\n",
    "plt.ylabel('Water floeing out of the dam (Y)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning goes as far as deep learning. The limit is the single layer neural network. With this in mind, let's return to the punishment process introduced in lab L04, now using neural network packages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linearized regression with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a network corresponding to a linear regression, which in our case will be based on 1 input variable and will consist of 1 layer containing 1 neuron. The BIAS tag will be added naturally and a penalty tag will be added. Two commonly used penalty methods are L1 and L2.\n",
    "\n",
    "#### L2 regularizáció (Ridge regression)\n",
    "$ C(w)=\\frac{1}{2m}\\sum_{i=1}^m(h_w(x^i)-y^i)^2+ \\color{red}{\\lambda\\sum_{j=1}^nw_j^2} $\n",
    "\n",
    "This technique can be used to prevent over fitting. $\\lambda $  should be choosen wisely. Setting it too high can result in under fitting. \n",
    "\n",
    "#### L1 regularisation (Lasso regression)\n",
    "$ C(w)=\\frac{1}{2m}\\sum_{i=1}^m(h_w(x^i)-y^i)^2+ \\color{red}{\\lambda\\sum_{j=1}^n|w_j|} $\n",
    "\n",
    "This technique can also reduce certain coefficients to 0, so it is useful for input parameter selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vizsgáljuk meg az alábbi háló összeállítást."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Underfit Case\n",
    "Lambda = 0\n",
    "lr_rate = 0.005\n",
    "epoch = 100\n",
    "\n",
    "# building a linear stack of layers with the sequential model\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_shape=(1,), use_bias=True, kernel_regularizer=regularizers.l2(Lambda)))\n",
    "\n",
    "# compiling the sequential model\n",
    "sgd = optimizers.Adam(lr=lr_rate)\n",
    "model.compile(loss='MSE', optimizer=sgd)\n",
    "\n",
    "# training the model and saving metrics in history\n",
    "history = model.fit(X_train, Y_train, epochs=epoch, validation_data=(X_val, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### CODE HERE ######################## \n",
    "# Implement the prediction step.\n",
    "# Use the predict() function of the model\n",
    "\n",
    "\n",
    "\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_train,Y_train,'x')   # adatok megjelenítése\n",
    "plt.plot(X_test,Y_pred,'x', color='red')   # adatok megjelenítése\n",
    "plt.title('Training data')\n",
    "plt.xlabel('Change in water level (X)')\n",
    "plt.ylabel('Water floeing out of the dam (Y)')\n",
    "plt.legend(['Training data', 'Prediction (on Test data)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In line with our expectations and our simple model design, we obtained a linear estimate that does not seem to adequately capture our data. Using the built-in metrics, we can examine the running of the cost function on the training and validation data. To do this, we first plot the corresponding metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss functions')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Training loss', 'Validation loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph, we can observe that both the cost function of the training data and the cost function of the validation data remain high. Higher than one would expect from a well-functioning predictor.\n",
    "\n",
    "<img src=\"assets/Lab08/Pics/L08_HighBias.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's change our model to get a better result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Poly Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a better result, we need more input parameters. One possible way to generate more parameters is to take the polynomial coefficients of the available parameter and feed them into your neural network as additional parameters.\n",
    "\n",
    "Write a function that takes as input the matrix of actual samples and enriches it with the appropriate columns up to the desired exponent. $x => x, x^2, x^3 ... x^p$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X,p=9):\n",
    "    ################### CODE HERE ######################## \n",
    "    # Implement the feature extension for the input data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    ######################################################    \n",
    "    return X_poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not forget to lower the values, otherwise the high exponent members would distort the learning very much. Write a normalization function that takes a sample matrix as input and returns the means and variances as a vector in addition to the normalized matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureNormalize(X):\n",
    "    ################### CODE HERE ######################## \n",
    "    # Normalize the features of the input data.\n",
    "    # Each features has different normalization values.\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ######################################################    \n",
    "    return X_norm,avg,std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform both the polynomial variable enrichment and the normalization. \n",
    "\n",
    "$\\color{red}{Warning!}$\n",
    "\n",
    "You should be aware of that during training you only want to use information from the training data set. This means that we should also normalise the test and validation data using the mean and standard deviation calculated from the training data. Otherwise, we would leak data back to the algorithm that could distort the learning result. The validation and test datasets thus remain truly independent and the algorithm can only work with the information obtained from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order=9\n",
    "\n",
    "# Training data\n",
    "X_train_p = polyFeatures(X_train, order)                                # polynomial features\n",
    "X_train_pn, mu, sigma = featureNormalize(X_train_p)                     # feature normalization\n",
    "\n",
    "################### CODE HERE ########################  \n",
    "# Perform the feature extension and normalization for the remaining data sets as well\n",
    "\n",
    "# Validation data\n",
    "\n",
    "\n",
    "# Teszt data\n",
    "\n",
    "\n",
    "###################################################### \n",
    "\n",
    "print(\"\"\"Expected Normalized Training Example for order=3 (approx.):\n",
    "[-0.362 -0.755  0.182 ]\"\"\")\n",
    "print('Normalized Training Example 1:\\n',X_train_pn[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just Right Case\n",
    "Lambda = 0\n",
    "lr_rate = 0.01\n",
    "epoch = 100\n",
    "optim = optimizers.SGD(lr=lr_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have increased the number of variables, we need to modify our model. The input layer should fit the size of our increased input matrix, but we still only want one output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a linear stack of layers with the sequential model\n",
    "model2 = Sequential()\n",
    "\n",
    "################### CODE HERE ######################## \n",
    "# Build a 2 layer Neural Network matching the input shape of the data and producing 1 output.\n",
    "# Use BIAS and L2 regularization.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the architecture of the model is established, the model is compiled and the cost function and the optimizer are defined. Again, there are several options to choose from, such as SGD or ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the sequential model\n",
    "# optim = optimizers.SGD(lr=lr_rate)\n",
    "optim = optimizers.Adam(lr=lr_rate)\n",
    "model2.compile(loss='MSE', optimizer=optim)\n",
    "\n",
    "# Training the model and saving metrics in history\n",
    "history2 = model2.fit(X_train_pn, Y_train, epochs=epoch, validation_data=(X_val_pn, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, we make this prediction on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### CODE HERE ########################\n",
    "# Calculate the predictions using the recently trained model.\n",
    " \n",
    "\n",
    "\n",
    "###################################################### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result of the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test,Y_pred2,'x', color=\"red\") \n",
    "plt.plot(X_test,Y_test,'x', color=\"green\") \n",
    "plt.title('Training data')\n",
    "plt.xlabel('Change in water level (X)')\n",
    "plt.ylabel('Water floeing out of the dam (Y)')\n",
    "plt.legend(['Prediction', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the extended model gives a much better result, which fits our data much better. The result is also reflected in the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(history2.history['loss'])\n",
    "plt.plot(history2.history['val_loss'])\n",
    "plt.title('Loss functions')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Validation loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the cost function of our training data and the cost function of our validation data were significantly lower than in our first attempt.\n",
    "\n",
    "<img src=\"assets/Lab08/Pics/L08_HighBias.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can extend your model further and tune the hyperparameters, but be careful not to fall into the trap of over-learning, as in the model below.\n",
    "\n",
    "In our case, we chose too many input parameters and ran the learning that way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Overfit\n",
    "Lambda = 0\n",
    "lr_rate = 0.001\n",
    "epoch = 100\n",
    "order=12\n",
    "optim = optimizers.Adam(lr=lr_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "X_train_p = polyFeatures(X_train, order)                                # polynomial features\n",
    "X_train_pn, mu, sigma = featureNormalize(X_train_p)                     # feature normalization\n",
    "\n",
    "# Validation\n",
    "X_val_p = polyFeatures(X_val, order)\n",
    "X_val_pn = (X_val_p-mu)/sigma\n",
    "\n",
    "# Teszt\n",
    "X_test_p = polyFeatures(X_test, order)\n",
    "X_test_pn = (X_test_p-mu)/sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a linear stack of layers with the sequential model\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(order, input_shape=(order,), use_bias=True, kernel_regularizer=regularizers.l2(Lambda)))\n",
    "model3.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the sequential model\n",
    "optim = optimizers.SGD(lr=lr_rate)\n",
    "model3.compile(loss='MSE', optimizer=optim)\n",
    "\n",
    "# training the model and saving metrics in history\n",
    "history3 = model3.fit(X_train_pn, Y_train, epochs=epoch, validation_data=(X_val_pn, Y_val), verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred3 = model3.predict(X_test_pn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_test,Y_pred3,'x', color=\"red\") \n",
    "plt.plot(X_test,Y_test,'x', color=\"green\") \n",
    "plt.title('Training data')\n",
    "plt.xlabel('Change in water level (X)')\n",
    "plt.ylabel('Water floeing out of the dam (Y)')\n",
    "plt.legend(['Prediction', 'Test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "plt.plot(history3.history['loss'])\n",
    "plt.plot(history3.history['val_loss'])\n",
    "plt.title('Loss functions')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Validation loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the model learns too much on the training samples and after the 60th epoch the cost function of the training samples continues to decrease, but the result on the validation samples starts to deteriorate. This means that although the cost function of the training samples performs well below an expected level, the cost function of the validation and test samples will be well above the expected result. The \"scissors are open\", there will be a large gap between the two cost functions.\n",
    "\n",
    "<img src=\"assets/Lab08/Pics/L08_HighVariance.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Summary and Tips\n",
    "\n",
    "Underfitting (High Bias problem):\n",
    "- Adding more input parameters to the model (either independent or polynomial).\n",
    "- $\\lambda$ parameter reduction, i.e. less penalization.\n",
    "- Increase the epoch number.\n",
    "- Increase $\\alpha$ (learning rate).\n",
    "\n",
    "Overfitting (High Variance problem):\n",
    "- Less input parameter.\n",
    "- Increase the $\\lambda$ parameter, i.e. we penalize more.\n",
    "- More learning samples.\n",
    "- Reducing $\\alpha$ (learning rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
