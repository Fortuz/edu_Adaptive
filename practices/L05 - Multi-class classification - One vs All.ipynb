{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L05%20-%20Multi-class%20classification%20-%20One%20vs%20All_solved.ipynb)\n",
    "\n",
    "# Labor 05: Multi-class classification - One vs All\n",
    "\n",
    "### Handwritten numbers I.\n",
    "\n",
    "In this exercise, we will use logistic regression to classifie handwritten digits (between 0 and 9). This is a common task nowadays, from reading postal codes to recognising numbers written on bank accounts and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Data load and visualisation\n",
    "\n",
    "The MNIST dataset will be used which is included in the Keras framework. The dataset contains 60000 sample images of 28x28 pixels. Since pixel positions won't be used the data will be flattened for simple data handling, In the matrix, one sample corresponds to one row, for which the images are expanded. Also the pixel values are normalized between 0 and 1. An example of the expansion is shown below.\n",
    "\n",
    "<img src=\"assets/Lab05/Pics/L05_Flatten.png\" width=450>\n",
    "\n",
    "Accordingly, scanning the data will give us a 5000x748 input matrix and a 5000x1 matrix containing the labels.\n",
    "\n",
    "To display the images, we need to resample them accordingly. Later in our algorithm, each pixel will be counted as an input variable, and each pixel value will be a grayscale value normalized between 0-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_data, y_data), _ = mnist.load_data()\n",
    "\n",
    "X = (x_data[0:5000, :, :].reshape((5000, -1)))/256            # flatten and normalization                                            \n",
    "Y = y_data[0:5000].reshape((5000, -1))                                           \n",
    "m = X.shape[0]                                                      \n",
    "\n",
    "print('''Shape of the dataset in order X and Y:\n",
    "''',X.shape,'\\n',Y.shape,'\\n')\n",
    "\n",
    "print(\"Now showing some random data from the dataset ...\")          \n",
    "fig, axis = plt.subplots(10,10,figsize = (8,8))                     \n",
    "for i in range(10):                                                 \n",
    "    for j in range(10):\n",
    "        axis[i,j].imshow(X[np.random.randint(0,m),:].reshape(28,28,order=\"C\"),cmap=\"hot\")   \n",
    "        axis[i,j].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Cost function and Gradient Descent\n",
    "\n",
    "Reminder: <br>\n",
    "In previous exercises we introduced the following cost function, which was used to separate 2 classes. We only distinguished between $y=0\\ and y=1$.\n",
    "\n",
    "$ C(w)=-\\frac{1}{m}\\sum_{i=1}^{m}y^i\\cdot log(h_w(x^i))+(1-y^i)\\cdot log(1-h_w(x^i))+\\frac{\\lambda}{2m}\\sum_{j=1}^nw_j^2 $ \n",
    "\n",
    "The derivative of the cost function for the gradient method is calculated using the following formula:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j}C(w)=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_j^i+\\frac{\\lambda}{m}w_j $ <br>\n",
    ", where we make sure that the BIAS member is not penalised!\n",
    "\n",
    "Then the weights were modified using the basic formula of the Gradient Descent weight modifier:\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w)$ \n",
    "\n",
    "These three key steps make up the Gradient Descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function lrCostFunction() that computes the cost function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))                                                 \n",
    "\n",
    "def lrCostFunction(w,X,Y,Lambda):                                           \n",
    "    ################### CODE HERE ######################## \n",
    "    # Implement a Cost function incorporating the learning rate parameter\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "    #######################################################\n",
    "\n",
    "    return C, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test case\n",
    "w_t = np.array([-2,-1,1,2]).reshape(4,1)                                 \n",
    "X_t = np.array([np.linspace(0.1,1.5,15)]).reshape(3,5).T                  \n",
    "X_t = np.hstack((np.ones((5,1)),X_t))                                    \n",
    "Y_t = np.array([1,0,1,0,1]).reshape(5,1)  \n",
    "Lambda = 3\n",
    "\n",
    "C,grad = lrCostFunction(w_t, X_t, Y_t, Lambda)                          \n",
    "print('\\nTest weight: [-2 -1 1 2]\\n''''\n",
    "Testing lrCostFunction() with regularization ... \n",
    "Cost function value: %.6f''' % C, ' (Expected: 2.534819)')\n",
    "print('''\\nExpected gradients:\n",
    " [[0.146561]\n",
    " [-0.548558]\n",
    " [0.724722]\n",
    " [1.398003]]\n",
    "Computed gradients:\\n''',grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the function gradientDescent() that performs gradient descent. In each iteration, store the current cost value in a variable to keep track of the run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,w,learning_rate,num_iters,Lambda):                        \n",
    "    ################### CODE HERE ########################    \n",
    "    # Implemen the Gradient Descent algorithm\n",
    "    # Use the previously implemented lrCostFunction()\n",
    "     \n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    #####################################################\n",
    "       \n",
    "    return w, C_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: One vs All\n",
    "\n",
    "Our goal is to extend the 2 class classifier, which we have already learned and coded, to split multiple classes. We can do this along a simple consideration. Let us derive any classification problem to separate two classes and perform this procedure as many times as we want to separate the classes.\n",
    "\n",
    "<img src=\"assets/Lab05/Pics/L05_OneVSAll.png\" width=450>\n",
    "\n",
    "Thus, our hypothesis function assigns to each sample as many probabilities as we have classes. Given this, our decision will be which class will have the highest probability.\n",
    "\n",
    "$h_{w}^{(i)} = P(y=i|_{x,w})$ <br>\n",
    ",where the illustration above shows $i = 1,2,3$.\n",
    "\n",
    "When you divide the numbers, you will have 10 divisions, as the digits range from 0 to 9. The thing to note is that in the database, the digits 0 are marked with 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneVsAll(X, Y, num_labels, Lambda):                                    \n",
    "    m, n = X.shape[0],X.shape[1]                                           \n",
    "    w_init = np.zeros((n + 1, 1))                                           \n",
    "    w_all = []  \n",
    "    C_all = []\n",
    "\n",
    "    ################### CODE HERE ########################               \n",
    "    # Implement the One vs All algorithm\n",
    "    # Do not forget to add the BIAS to the input X data\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "    ######################################################\n",
    "    #    \n",
    "    return w_all, C_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the One Vs ALL algorithm. Experiment with different $\\lambda$ parameter settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0.1\n",
    "num_labels = 10\n",
    "\n",
    "w_all, C_all = oneVsAll(X,Y,num_labels,Lambda)\n",
    "\n",
    "plt.plot(C_all[0])\n",
    "plt.title(\"Cost function for y = 1 (Digit 1)\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"C_all value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Prediction and Accuracy\n",
    "\n",
    "Let's look back at how accurate our trained algorithm would be on the training samples. We now perform the prediction step in a batch method on all samples at once. For each sample, we compute the probability of belonging to all classes, and then select the class with the highest probability among them for our final prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictionOneVsAll(w_all, X):\n",
    "    m = X.shape[0]\n",
    "    X = np.hstack((np.ones((m,1)),X))\n",
    "\n",
    "    ################### CODE HERE ########################\n",
    "    # Implement the prediction step for the One Vs All method\n",
    "    # Calculate the prediction for every class give an input\n",
    "    # Choose the class with the highet probability as a prediction\n",
    "    # Decode the choosen class for a real number representing the prediction.\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "         \n",
    "        \n",
    "    ######################################################\n",
    "      \n",
    "    return predictions              \n",
    "\n",
    "pred = predictionOneVsAll(w_all, X)\n",
    "acc = sum(pred[:,np.newaxis]==Y)[0]/5000*100\n",
    "print(\"Training Set Accuracy: %.2f\" % acc,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
