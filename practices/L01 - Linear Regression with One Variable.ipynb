{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L01%20-%20Linear%20Regression%20with%20One%20Variable.ipynb)\n",
    "\n",
    "# Lab 01: Linear regression with one variable\n",
    "### Profit in a city\n",
    "In this exercise, we will implement a univariate linear regression to predict the profit of a food supply unit.\n",
    "\n",
    "Imagine we are CEOs of a food chain and we are thinking about where to open a new store, in which cities it would be worthwhile for us. The grocery chain already has the conditions to do this, we just need to decide which cities to expand in. To do this, we'll use the data the company has collected on the population of the cities and the expected profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import the packages\n",
    "We will need some elements of matplotlib for drawing and NumPy for easier array handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Data load from file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to have write the opening and closing of the data in one cell. Otherwise, it may cause unexpected errors. \n",
    "\n",
    "If the data file is next to the main file, the load action is very simple.  For more complex file structures, the whole path must be specified.  The raw, unsorted data is then stored in a variable.  \n",
    "\n",
    "Optional: plot the result to see what the next step should be to sort the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=open(\"Lab1data.txt\",'r')\n",
    "lines=file.readlines()\n",
    "file.close()                    \n",
    "\n",
    "#print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sort your data into the appropriate variables. First we define 2 lists, X and Y. Moving down the line, we split the data along the separator character. Then we transform the data into a NumPy array and format it into 2 column vectors. At the end of the operation, the dimension of both arrays will be m x 1, where m is the number of samples.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]                               # List definitions\n",
    "Y=[]\n",
    "for line in lines:                 # Go through the scanned data line by line\n",
    "  a,b = line.split(',')            # Split according to the separating character\n",
    "  X.append(float (a))\n",
    "  Y.append(float(b))\n",
    "\n",
    "X=np.array(X)                      # Convert the data into numpy arrays\n",
    "Y=np.array(Y) \n",
    "m=Y.size\n",
    "X=X.reshape(m,1)                   # For format matching, we also treat the column vectors as mx1 matrices\n",
    "Y=Y.reshape(m,1) \n",
    "\n",
    "print('X dimension:', X.shape, '; X values:\\n', X, '\\n')                \n",
    "print('Y dimension:', Y.shape, '; Y values:\\n', Y, '\\n')\n",
    "print('Number of data: ', m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data on a graph to see the structure of the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X,Y,'o', c= \"g\")   \n",
    "plt.title(\"Training data\")\n",
    "plt.xlabel(\"Population of a city in 10 000s\")\n",
    "plt.ylabel(\"Profit in $10 000s \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for example, the graph shows that a city with a population of 150,000 can expect to make a profit of about $120,000. This is our labelled data, so each sample has its expected output. In our example, the population data will be the input and the expected profit will be the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: A hypothesis: Linear regression model\n",
    "We will try to approximate the data on the graph with a line, which will be our starting hypothesis.\n",
    "Our linear regression model can be written as follows:\n",
    "\n",
    "$ h_{w}(x) = w_{0} + xw_{1}  = \\hat{y}$\n",
    "\n",
    ",where <br>\n",
    "$h$ - hypothesis<br>\n",
    "$x$ - input data - so called feature. (Only one in this model)<br>\n",
    "$w$ - weights <br>\n",
    "$\\hat{y}$ - prediction\n",
    "\n",
    "The block diagram of the algorithm is illustrated in the figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab01/Pics/L01_Model.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal is to get the expected profit output given the size of a city according to our hypothesis. To do this, we first need to set up our hypothesis. First, we specify the model, which in our case is a linear fit, and then we set the parameters using the available samples, i.e. we train it.\n",
    "\n",
    "To test the goodness of fit of our model, we need to define a cost function. The cost function will determine how good our model is in a given iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function (also called Error function) is the MSE (Mean Squared Error) function, which can be written as follows:\n",
    "\n",
    "$C=\\frac{1}{2m}\\sum_{i=1}^{m}(\\hat{y}^{i}-y^{i})^2$\n",
    "\n",
    "This is the sum of the squared differences between our estimates and our labels for the samples. Thus, the error will always be a positive number, the smaller the better our estimate. The constant multiplier term corresponding to the sample number will play more of a role later on. The nature of the error term is not affected.\n",
    "\n",
    "Inserting our hypothesis we obtain the following form:\n",
    "\n",
    "$C=\\frac{1}{2m}\\sum_{i=1}^{m}(h_w(x^{i})-y^{i})^2$\n",
    "\n",
    "And in its final complete form:\n",
    "\n",
    "$ C(w_{0},w_{1})=\\frac{1}{2m} \\cdot \\sum_{i=1}^{m}(w_{0} + x^iw_{1} − y^{i})^2 $\n",
    "\n",
    "As we have defined our hypothesis in a way that we can adjust the offset of the line with the weight $w_0$ in addition to the slope of the line, it is useful to introduce a variable to facilitate matrix operations. Let $x_0=1$, so we have extended our single variable input with a BIAS member. \n",
    "\n",
    "$ C(w_{0},w_{1})=\\frac{1}{2m} \\cdot \\sum_{i=1}^{m}(w_{0}x^{i}_{0} + w_{1}x^{i}_{1} − y^{i})^2 $\n",
    "\n",
    "With this extension, where the constans 1 member is also considered as an input variable, the matrix specification is facilitated. \n",
    "\n",
    "In matrix form, we can write the cost function as follows:\n",
    "\n",
    "$ C = \\frac{1}{2m} \\cdot \\sum(XW-Y)^2 $\n",
    "\n",
    "Let's review the matrices used in this example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab01/Pics/L01_Matrixok.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the help of these matrices, our estimation per sample and the calculation of the cost function for all samples can be done easily.\n",
    "\n",
    "<img src=\"assets/Lab01/Pics/L01_CostCalculation.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the necessary matrices! Initialize the weights with 0!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init W, Extend X\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the dimension of your arrays before performing any matrix operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X dimensions:\\n', X.shape)                \n",
    "print('Y dimensions:\\n', Y.shape)\n",
    "print('W dimensions:\\n', W.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost function:\n",
    "Define a function to compute the cost function that takes as input the matrices $X$, $Y$ and $W$ and returns the computed scalar cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeCost(X,Y,W):\n",
    "######################################################   \n",
    "\n",
    "\n",
    "######################################################\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1=computeCost(X,Y,np.array([[0],[0]]).reshape((2,1)))\n",
    "print('''Test (Cost function):\n",
    "\\tWeights: W = [0;0]\n",
    "\\tApproximate ground trueth value = 32.07\n",
    "\\tCalculated value = ''',C1)\n",
    "C2=computeCost(X,Y,np.array([[-1],[2]]))\n",
    "print('''\\n\\tWeights: W = [-1;2]\n",
    "\\tApproximate ground trueth value = 54.24\n",
    "\\tCalculated value = ''',C2)\n",
    "\n",
    "if int(C1) == 32 and int(C2) ==54:\n",
    "    print(\"\\n The computeCost function works properly.\")\n",
    "else:\n",
    "    print(\"\\n Something went wrong!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the cost function, the task is to minimize the cost function. We aim for an optimum. To find the minimum point we will use the graph method.\n",
    "\n",
    "The gradient descent method consists of calculating the gradient (derivative) of the function at a given point. The derivative in our case effectively defines the tangent line of the function at that point. Accordingly, we can determine where we need to move on the function to get closer to the minimum. At the minimum point, the derivative of the function is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab01/Pics/L01_Gradient.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the gradient (partial derivatives) of the cost function to modify the weights. The modifications are continued until the algorithm converges to the minimum point.\n",
    "\n",
    "The weight update of the gradient method can be described by the following formula: \n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w_0,w_1)$ \n",
    "\n",
    ",where <br>\n",
    "$\\mu$ - learning rate, parameter to set the speed of convergence. $0 < \\mu < 1$.\n",
    "\n",
    "When $\\mu$ is chosen small, convergence slows down but approaches the minimum point more closely <br>\n",
    "When $\\mu$ is chosen high, it can cause divergence, which can lead to the algorithm crashing.\n",
    "\n",
    "One thing to keep in mind when using this method is that the weights should be adjusted simultaneously and in synchrony.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Gradient descent applied to linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis and cost function of the linear regression model:\n",
    "\n",
    "$ h_w(x)=w_0+w_1x $\n",
    "\n",
    "$ \\color{blue}{C(w_0,w_1)}=\\frac{1}{2m}\\sum_{i=1}^{m}(h_w(x^{i})-y^{i})^2 $\n",
    "\n",
    "Weight update of the gradient descent method:\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}\\color{blue}{C(w_0,w_1)}$ \n",
    "\n",
    "Substituting the linear regression model into the gradient descent method for the partial derivative, we obtain the following:\n",
    "\n",
    "$ \\frac{\\partial}{\\partial w_j}C(w_0,w_1)=\\frac{\\partial}{\\partial w_j}\\frac{1}{2m}\\sum_{i=1}^{m}(h_w(x^{i})-y^{i})^2\n",
    "=\\frac{\\partial}{\\partial w_j}\\frac{1}{2m}\\sum_{i=1}^{m}(w_0+w_1x^i-y^{i})^2 $\n",
    "\n",
    "Performing the partial derivation according to the weights.\n",
    "\n",
    "$ \\color{red}{(j=0)}\\hspace{7mm} \\frac{\\partial}{\\partial w_j}C(w_0,w_1)=\\frac{1}{m}\\sum_{i=1}^{m}(w_0+w_1x^i-y^{i})\\cdot 1=\\frac{1}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i})\\cdot \\color{red}{x_0^i}) $\n",
    "\n",
    "$ \\color{red}{(j=1)}\\hspace{7mm} \\frac{\\partial}{\\partial w_j}C(w_0,w_1)=\\frac{1}{m}\\sum_{i=1}^{m}(w_0+w_1x^i-y^{i})\\cdot x_1^i=\\frac{1}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i})\\cdot x_1^i) $\n",
    "\n",
    "So accordingly, the weight updates in our example are given by:\n",
    "\n",
    "$ w_0=w_0-\\frac{\\mu}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i}) \\cdot \\color{red}{x_0^i}) $\n",
    "\n",
    "$ w_1=w_1-\\frac{\\mu}{m}\\sum_{i=1}^{m}((h_w(x^{i})-y^{i}) \\cdot x^i_1) $\n",
    "\n",
    "Since we introduced $ x_0 $ as a variable, the weight update formula generalizes well to different cases. Care should also be taken to update the weights simultaneously during the encoding, as asynchronous updating may cause a counting error!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A folyamatot addig folytatjuk, míg el nem érjük az optimum pontot vagy el nem érünk egy kívánt iterációs számot. Definiáljunk tehát egy tanulási rátát és egy epoch számot, amely a kívánt frissítések számát fogja limitálni."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 1500                       \n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that implements the gradient method. Within the function, store the previous values of our cost function in the variable $C_{history}$ for later evaluation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X, Y, W, learning_rate, epochs):\n",
    "    m = Y.size\n",
    "    C_history = np.zeros((epochs,1))\n",
    "######################################################    #Warning: simultaneous update!!!!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "    return W,C_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our function using the gradient descent method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''\\n gradientDescent() függvény teszt (learning_rate=0.01):\n",
    "\\tWeights expected (approx.): \n",
    "\\t [-3.6303] [1.1664]''')\n",
    "W,C_history=gradientDescent(X,Y,W,learning_rate,epochs)\n",
    "print('\\tWeights calculated:\\n\\t',W[0],W[1])\n",
    "\n",
    "if (W[0]+3.6303) < 0.1 and (W[1]-1.1664) < 0.1:\n",
    "    print(\"\\n A gradientDescent függvény megfelelő. Tovább mehet.\")\n",
    "else:\n",
    "    print(\"\\n Valami nem stimmel. Korrekció szükséges!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Visualization\n",
    "### Display cost function during iterations:\n",
    "Plot the values of the cost function over the iterations. This will show how our algorithm actually converges during the epoch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C_history)\n",
    "plt.title(\"Cost function of Gradient descent algorithm\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the fitted line on the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot((X[:,1]).reshape(97,1),Y,'o', label = \"Training data\")\n",
    "plt.plot((X[:,1]).reshape(97,1),X@W,'-',label = \"Linear regression\")\n",
    "plt.xlabel(\"Population of a city in 10 000s\")\n",
    "plt.ylabel(\"Profit in $10 000s \")\n",
    "plt.title(\"Linear regression and the training data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Prediction with the trained model\n",
    "To use the model, estimate the expected profit in a city of 10000 and 17000 inhabitants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Prediction1 = (np.array([1, 10]))@W\n",
    "Prediction2 = (np.array([1, 17]))@W\n",
    "print('\\nPrediction for 10 000 citizens: %.2f $' % (Prediction1 * 10000))\n",
    "print('\\nPrediction for 17 000 citizens: %.2f $' % (Prediction2 * 10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display cost function (surface plot)\n",
    "For a better illustration, it is worth examining the cost function over a larger interval. To do this, create a vector for each of our two weights and calculate the cost for each pair of values. This will give us an idea of the surface on which the optimum point was sought and where it was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0_vals = np.linspace(-10,10,100)\n",
    "w1_vals = np.linspace(-1,4,100)\n",
    "C_vals = np.zeros((w0_vals.size,w1_vals.size))\n",
    "\n",
    "for i in range((w0_vals).size):\n",
    "    for j in range((w1_vals).size):\n",
    "        t=np.array([w0_vals[i],w1_vals[j]]).reshape(2,1)\n",
    "        C_vals[[i],[j]]= computeCost(X,Y,t)\n",
    "C_vals=C_vals.T\n",
    "\n",
    "fig= plt.figure()\n",
    "ax=plt.axes(projection='3d')\n",
    "x, y = np.meshgrid(w0_vals, w1_vals)\n",
    "surf = ax.plot_surface(x, y, C_vals, cmap=cm.coolwarm,linewidth=0, antialiased=False)\n",
    "plt.title(\"Surface plot of the Cost function\")\n",
    "plt.xlabel(\"w0\")\n",
    "plt.ylabel(\"w1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display cost function (contour plot)\n",
    "The contour plot is also a useful visualisation option, which has the advantage of flattening our 3D surface into 2D for easier transparency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.contour(w0_vals,w1_vals,C_vals,np.logspace(-2,3,20))\n",
    "plt.plot(W[0],W[1],'x')\n",
    "plt.title(\"Contour plot of C_vals in logarithmic scale\")\n",
    "plt.xlabel(\"w_0\")\n",
    "plt.ylabel(\"w_1\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once again. Using higher level packages:\n",
    "Python and its advanced packages allow you to write much more compact code, making it easier to prototype quickly. Let's see how the example we looked at in the exercise can be more concisely solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd                                  # Pandas for data handling\n",
    "from sklearn.linear_model import LinearRegression    # sklearn implementation\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('Lab1data.txt', header = None)    # Load data\n",
    "XX = data.iloc[:, 0].values.reshape(-1, 1)           # Separate data\n",
    "YY = data.iloc[:, 1].values.reshape(-1, 1)           # Separate data\n",
    "\n",
    "\n",
    "lin_reg = LinearRegression()                         # Set linear regression class\n",
    "lin_reg.fit(XX,YY)                                   # Fit linear regression\n",
    "pred1 = lin_reg.predict([[10]])                      # Prediction for 10000\n",
    "pred2 = lin_reg.predict([[17]])                      # Prediction for 17000\n",
    "\n",
    "print('Prediction for a city with a population of 10 000: %.2f $' % (pred1*10000))\n",
    "print('Prediction for a city with a population of 17 000: %.2f $' % (pred2*10000))\n",
    "\n",
    "regline = lin_reg.predict([[5],[22.5]])              # Need to the linear regression equatation   \n",
    "\n",
    "plt.scatter(XX,YY, label = \"Training data\")          # Plot\n",
    "plt.plot([[5],[22.5]],regline,'r')\n",
    "plt.xlabel(\"Population of a city in 10 000s\")\n",
    "plt.ylabel(\"Profit in $10 000s \")\n",
    "plt.title(\"Linear regression and the training data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
