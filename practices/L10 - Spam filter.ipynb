{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L10%20-%20Spam%20filter_solved.ipynb)\n",
    "\n",
    "# Labor 10 - Spam filter\n",
    "\n",
    "In this lab exercise we will use the SVM algorithm to filter spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "from nltk.stem import PorterStemmer             # natural langage toolkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load in Data\n",
    "\n",
    "The data will be loaded from a publicly available file. An alternative solution would be to upload the data file directly to the google colab file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab10/emailSample1.txt\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab10/emailSample2.txt\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab10/vocab.txt\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab10/spamSample1.txt\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab10/spamSample2.txt\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab10/spamTest.mat\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab10/spamTrain.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read in our data. We will work with two emails and a dictionary using the normalised form of the most commonly used terms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mail1 = open(\"emailSample1.txt\",\"r\").read()     # load first mail\n",
    "mail2 = open(\"emailSample2.txt\",\"r\").read()     # load second email\n",
    "vocabList = open(\"vocab.txt\",\"r\").read()        # load vocabulary\n",
    "\n",
    "print('First mail:')                            \n",
    "print(mail1)\n",
    "print('Second mail:')\n",
    "print(mail2)\n",
    "print('Vocabulary list:')\n",
    "print(vocabList)\n",
    "\n",
    "vocabList=vocabList.split(\"\\n\")[:-1]            # reshape the vocabulary\n",
    "vocabList_d={}\n",
    "for ea in vocabList:\n",
    "    value,key = ea.split(\"\\t\")[:]\n",
    "    vocabList_d[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23: Email preprocess\n",
    "\n",
    "The first step is to normalise the text of the email. What does this mean?\n",
    "- Convert everything to lower case\n",
    "- Extract the HTML code\n",
    "- Normalising URLs\n",
    "- Normalise the numbers\n",
    "- Normalize email addresses\n",
    "- Normalize special characters\n",
    "- We reduce words to a dictionary form\n",
    "- Omit numerals (single-letter characters)\n",
    "\n",
    "In most cases, normalisation will mean replacing an element with a simplified string.\n",
    "\n",
    "After normalization, we will decode the email into a sequence of numbers based on our dictionary. That is, we return the index of the words in the email from our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processEmail(mailcontent):\n",
    "    word_indices=[]                                                         # initialization\n",
    "    \n",
    "    mailcontent = mailcontent.lower()                                       # lowercase\n",
    "    mailcontent = re.sub(\"[http|https]://[^\\s]*\",\"httpaddr\",mailcontent)    # HTML normalization\n",
    "    mailcontent = re.sub(\"[^\\s]+@[^\\s]+\",\"emailaddr\",mailcontent)           # email address normalization \n",
    "    mailcontent = re.sub(\"[0-9]+\",\"number\",mailcontent)                     # nomber normalization\n",
    "    specChar = [\"<\",\"[\",\"^\",\">\",\"+\",\"?\",\"!\",\"'\",\".\",\",\",\":\",\"$\"]            # special character list\n",
    "    \n",
    "    ################### CODE HERE ######################## \n",
    "    # Normalize special characters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    \n",
    "    ps = PorterStemmer()                                                    # natural language processing - dictionary form reduction\n",
    "    mailcontent = [ps.stem(token) for token in mailcontent.split(\" \")]\n",
    "    mailcontent= \" \".join(mailcontent)\n",
    "    \n",
    "    mailcontent = mailcontent.replace(\"\\n\",\" \")\n",
    "    \n",
    "    ################### CODE HERE ######################## \n",
    "    # word_indices upload = Decode emails into a list of numbers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #####################################################\n",
    "    \n",
    "    print('Preprocessed mail:',mailcontent)\n",
    "    return word_indices\n",
    "\n",
    "word_indices = processEmail(mail1)\n",
    "print('\\nWord indices:',word_indices)\n",
    "\n",
    "check_email= [86, 916, 794, 1077, 883, 370, 1699, 790, 1822, 1831, 883, 431, 1171, 794, 1002, 1893, 1364, 592, 1676, 238, 162, 89, 688, 945, 1663, 1120, 1062, 1699, 375, 1162, 1120, 1893, 1510, 1182, 1237, 810, 1895, 1440, 1547, 181, 1699, 1758, 1896, 688, 1676, 992, 961, 1477, 71, 530, 1699, 531]\n",
    "if word_indices == check_email:\n",
    "    print(\"\\n Correct transformation. Proceed.\")\n",
    "else:\n",
    "    print(\"\\nSomething went wrong. Check your implementation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Feature extraction\n",
    "\n",
    "The next step is to create a features vector from the preprocessed email, the size of which is equal to the size of our dictionary and whichever word from the dictionary is in the email should have a feauture value of 1.\n",
    "\n",
    "<img src=\"assets/Lab10/Pics/L10_vector.png\" width=\"150\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emailFeatures(word_indices,vocabList):\n",
    "    ################### CODE HERE ########################   \n",
    "    # Make a featuer vector from the email. \n",
    "    # The dimensions of the vector should match the vocabList\n",
    "    # 1 represents a feature which is present in the email, and 0 otherwise.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "                                   \n",
    "    #####################################################\n",
    "    return features\n",
    "\n",
    "features = emailFeatures(word_indices,vocabList_d)\n",
    "print(\"Length of feature vector (1899 expected): %.0f\" % len(features))\n",
    "print(\"Number of non-zero entries (43 expected): %.0f\" % np.sum(features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Train SVM to classify Spams\n",
    "\n",
    "Using the training email train the SVM classifier using a linear kernel.\n",
    "\n",
    "If the hyperplane classifies the dataset linearly then the algorithm we call it as SVC (Support Vector Classifier) and the algorithm that separates the dataset by non-linear approach then we call it as SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mat = loadmat(\"spamTrain.mat\")\n",
    "X_train = spam_mat[\"X\"]\n",
    "y_train = spam_mat[\"y\"]\n",
    "C =0.2\n",
    "\n",
    "################### CODE HERE ########################\n",
    "#SVC initialization (kernel=linear), use the ravel() function \n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "print(\"Training Accuracy (99.975% expected):\",(spam_predictor.score(X_train,y_train.ravel()))*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Classification test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_mat2 = loadmat(\"spamTest.mat\")\n",
    "X_test = spam_mat2[\"Xtest\"]\n",
    "y_test = spam_mat2[\"ytest\"]\n",
    "\n",
    "print(\"Training Accuracy (98.9% expected):\",(spam_predictor.score(X_test,y_test.ravel()))*100,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Main indicators of a Spam\n",
    "\n",
    "Since the model we are training is a linear SVM, we can look at the individual weights that the model has learned during the classification process. In what follows, we implement a code snippet that shows which words (and their weights) the algorithm \"thinks\" are most likely to spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = spam_predictor.coef_[0]\n",
    "weights_col = np.hstack((np.arange(1,1900).reshape(1899,1),weights.reshape(1899,1)))\n",
    "weights_sorted = weights_col[weights_col[:,1].argsort()][::-1]\n",
    "\n",
    "spamvoc_ind = weights_sorted[0:15,0]\n",
    "spamvoc_weights = (weights_sorted[0:15,1])\n",
    "j=0\n",
    "for i in spamvoc_ind:\n",
    "    print(vocabList[int(i-1)],'\\t', '\\t', spamvoc_weights[j])\n",
    "    j=+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Test with own email\n",
    "\n",
    "For the sake of curiosity, you can also test it on your own email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ownmail = open(\"spamSample1.txt\",\"r\").read()\n",
    "own_ind = processEmail(ownmail)\n",
    "x = emailFeatures(own_ind,vocabList_d)\n",
    "\n",
    "p = spam_predictor.predict(x.reshape(1,-1))\n",
    "print(\"Result is:\",p)\n",
    "if (p==0):\n",
    "    print(\"This is NOT a SPAM\")\n",
    "elif(p==1):\n",
    "    print(\"This is a SPAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
