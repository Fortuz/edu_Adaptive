{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L09%20-%20SVM_solved.ipynb)\n",
    "\n",
    "# Labor 09 - Support Vektor Machine (SVM)\n",
    "\n",
    "## 0: Theoretical background\n",
    "\n",
    "For classification problems, Support Vector Machine (SMV) can be a good solution. SVM can be used to find the line that separates the data with the widest possible boundary for a linearly separable data set.\n",
    "\n",
    "For two linearly separable clusters, there are infinitely many linear boundaries. We want to choose the one that is furthest from the points of both clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab09/Pics/L09_SVMexample.png\" width=\"400\">\n",
    "\n",
    "Our two classes are marked in blue and red. The solid black line is our decision boundary. The highlighted points are the support vectors. The distance between the highlighted points and the decision boundary is the maximum of all possible variations in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the L03 lab, what it really means is that when we introduce our sigmoid function, we want to define not just a sharp cutoff, but a whole band."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$ h_w(x) = \\frac{1}{1+e^{-XW}} $ <br>\n",
    "\n",
    "$ h_w(x) = g(XW)$\n",
    "\n",
    "$ h_w(x) = g(z)$\n",
    "\n",
    "If $y=1$ we want $z\\geq1$.<br>\n",
    "If $y=0$ we want $z\\leq-1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab09/Pics/L09_SVMsigmoid.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cost function is defined in the following form:\n",
    "\n",
    "$ C(w) = \\frac{1}{m} \\sum{-y^i\\cdot\\log(h_w(x^i))-(1-y^i)\\cdot\\log(1-h_w(x^i))} $\n",
    "\n",
    ", where <br>\n",
    "\n",
    "$ h_w(x) = \\frac{1}{1+e^{-z}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce two new functions: $cost_{1}(z)$ és $cost_{0}(z)$ <br>\n",
    "so the new range is between 1 and -1. \n",
    "\n",
    "<img src=\"assets/Lab09/Pics/L09_newcost.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The minimisation of the cost function is still our task, modified as follows:\n",
    "\n",
    "$min_w\\ \\frac{1}{m} \\Big[ \\sum_{i=1}^m y^{(i)}\\cdot \\big(-log(h_w(x^{(i)})+(1-y^{(i)})\\big)\\cdot \\big(-log(1-h_w(x^{(i)}))\\big)\\Big] +\\frac{\\lambda}{2m}\\sum_{j=1}^nw_j^2 $\n",
    "\n",
    "$ min_w\\ {\\color{red} C} \\Big[ \\sum_{i=1}^m y^{(i)}\\cdot \\color{red} {cost_1}(h_w(x^{(i)})+(1-y^{(i)})\\big)\\cdot\\color{red} {cost_0}(1-h_w(x^{(i)}))\\big)\\Big] +\\frac{1}{2}\\sum_{j=1}^nw_j^2$\n",
    "\n",
    ", where $C = 1/\\lambda$ <br>\n",
    "<font color='red'>[WARNING! C in the formula is not the same as C(w) used to denote the cost function]. The notation is used because of the syntax used in the original python package. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the introduction seen previously in the L03-L04 labs, these considerations can therefore be used to extend the SVM method to non-linearly separable classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/Lab09/Pics/L09_SVMtransfer.png\" width=\"550\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernel trick for Linearly not separatable problems\n",
    "\n",
    "A good method for finding the most robust boundary for different classification problems is SVM. It is based on the concept of solving linearly separable problems, but can be made suitable for solving virtually arbitrary problems by using different kernels.\n",
    "\n",
    "The kernel trick is basically to transform your data into a higher dimension, where you can now separate them linearly.\n",
    "\n",
    "As an illustrative example, the 2D data set below is not linearly separable.\n",
    "\n",
    "<img src=\"assets/Lab09/Pics/L09_NonSep.png\" width=\"350\">\n",
    "\n",
    "Gaussian kernel (or later using Radial basis function kernel)\n",
    "\n",
    "<img src=\"assets/Lab09/Pics/L09_Gauss.png\" width=\"350\">\n",
    "\n",
    "The Gaussian kernel mathematically transforms our data into an infinite dimensional space. This allows us to obtain our solution from a higher dimensional space.\n",
    "\n",
    "The 3D representation of our data will now be a linearly separable data set.\n",
    "\n",
    "<img src=\"assets/Lab09/Pics/L09_Sep.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The theoretical summary was just a superficial overview of the mathematical background to get a general idea of what works in our code using higher-level Python packages in the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be loaded from a publicly available file. An alternative solution would be to upload the data file directly to the google colab file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab09/Lab9data1.mat\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab09/Lab9data2.mat\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab09/Lab9data3.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data! Use the scipy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat('Lab9data1.mat')\n",
    "X1 = data[\"X\"]\n",
    "Y1 = data[\"y\"]\n",
    "\n",
    "print(X1.shape)\n",
    "print(Y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Visualization\n",
    "\n",
    "Let's vizualise the data set to understand it more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataSeparation(X,Y):\n",
    "    CLR1 = []                                    # cluster1\n",
    "    CLR2 = []                                    # cluster2\n",
    "    \n",
    "    for i in range(Y.shape[0]):\n",
    "        if (Y[i] == 0):\n",
    "            CLR1.append(X[i,:])\n",
    "        elif (Y[i] == 1):\n",
    "            CLR2.append(X[i,:])\n",
    "            \n",
    "    CLR1 = np.asmatrix(CLR1)\n",
    "    CLR2 = np.asmatrix(CLR2)\n",
    "    return CLR1, CLR2\n",
    "\n",
    "CLR11,CLR12 = dataSeparation(X1,Y1)\n",
    "\n",
    "plt.plot(CLR11[:,0],CLR11[:,1],'gx')\n",
    "plt.plot(CLR12[:,0],CLR12[:,1],'ro')\n",
    "plt.title(\"First dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing our data, we can see that this data set is linearly separable. We use the SVM model, for which all the necessary algorithms are provided in the sklearn package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Train linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this task, you don't need any extra kernel (no need to fiddle around), so you can simply set the kernel parameter to linear.\n",
    "The C parameter is initially set to 0.5 (we will talk about the ravel() function later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(C=0.5,kernel=\"linear\")\n",
    "print(classifier)\n",
    "classifier.fit(X1,Y1.ravel())\n",
    "\n",
    "plt.plot(CLR11[:,0],CLR11[:,1],'gx')\n",
    "plt.plot(CLR12[:,0],CLR12[:,1],'ro')\n",
    "\n",
    "# plotting the decision boundary\n",
    "X_1,X_2 = np.meshgrid(np.linspace(X1[:,0].min(),X1[:,1].max(),num=100),np.linspace(X1[:,1].min(),X1[:,1].max(),num=100))\n",
    "desBoundary1 = classifier.predict(np.array([X_1.ravel(),X_2.ravel()]).T).reshape(X_1.shape)\n",
    "plt.contour(X_1,X_2,desBoundary1,1,colors=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that 1 point has been classified in the wrong group. At a glance, this point could be a measurement error. If you want your solution to handle this outlier correctly, you should tune parameter C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: C = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier2 = SVC(C=100,kernel=\"linear\")\n",
    "print(classifier2)\n",
    "classifier2.fit(X1,Y1.ravel())\n",
    "\n",
    "plt.plot(CLR11[:,0],CLR11[:,1],'gx')\n",
    "plt.plot(CLR12[:,0],CLR12[:,1],'ro')\n",
    "\n",
    "# plotting the decision boundary\n",
    "X_1,X_2 = np.meshgrid(np.linspace(X1[:,0].min(),X1[:,1].max(),num=100),np.linspace(X1[:,1].min(),X1[:,1].max(),num=100))\n",
    "desBoundary2 = classifier2.predict(np.array([X_1.ravel(),X_2.ravel()]).T).reshape(X_1.shape)\n",
    "plt.contour(X_1,X_2,desBoundary2,colors=\"b\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the task can still be solved, but the safety margin between the two clusters is significantly reduced. It could easily be a case of over-learning. \n",
    "\n",
    "Let's discuss how the SVM method can be applied to a more complex data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Loading and visualising a second data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################### CODE HERE ######################## \n",
    "# Load the second dataset (Lab9data2.mat)\n",
    "# Separate X and Y\n",
    "# print out the shapes of the data matrixes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################################\n",
    "\n",
    "CLR21,CLR22 = dataSeparation(X2,Y2)\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(CLR21[:,0],CLR21[:,1],'gx')\n",
    "plt.plot(CLR22[:,0],CLR22[:,1],'ro')\n",
    "plt.title(\"Second dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that this data set is not linearly separable. Kernelisation will be required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Gauss kernel\n",
    "\n",
    "The SVC function of the sklearn package has the RBF keyword as kernel parameter to select the Gauss gauge (RBF = Radial basis function)\n",
    "\n",
    "This is where the ravel() function comes in handy, as when transforming to a higher dimension, care must be taken to ensure that Y is also expanded to the correct dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier3 = SVC(kernel = \"rbf\", gamma=30)\n",
    "print(classifier3)\n",
    "classifier3.fit(X2,Y2.ravel())\n",
    "\n",
    "plt.figure(figsize=(10,7))\n",
    "plt.plot(CLR21[:,0],CLR21[:,1],'gx')\n",
    "plt.plot(CLR22[:,0],CLR22[:,1],'ro')\n",
    "plt.title('Második adathalmaz és a döntési határ')\n",
    "\n",
    "X_5,X_6 = np.meshgrid(np.linspace(X2[:,0].min(),X2[:,1].max(),num=100),np.linspace(X2[:,1].min(),X2[:,1].max(),num=100))\n",
    "desBoundary3 = classifier3.predict(np.array([X_5.ravel(),X_6.ravel()]).T).reshape(X_5.shape)\n",
    "\n",
    "plt.contour(X_5,X_6,desBoundary3,colors = \"b\")\n",
    "plt.xlim(0,1.05)\n",
    "plt.ylim(0.35,1.05)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Visualising a third data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"Lab9data3.mat\")\n",
    "X3 = data[\"X\"]\n",
    "Y3 = data[\"y\"]\n",
    "Xval = data[\"Xval\"]\n",
    "Yval = data[\"yval\"]\n",
    "print(X3.shape)\n",
    "print(Y3.shape)\n",
    "\n",
    "CLR31,CLR32 = dataSeparation(X3,Y3)\n",
    "\n",
    "plt.plot(CLR31[:,0],CLR31[:,1],\"go\")\n",
    "plt.plot(CLR32[:,0],CLR32[:,1],\"rx\")\n",
    "plt.title(\"Third dataset\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this dataset we will demonstrate the parameter tuning with some predefined constant C."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8: Parameter estimation with C values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset3Params(X, y, Xval, yval,vals):\n",
    "    acc = 0\n",
    "    best_c=0\n",
    "    best_gamma=0\n",
    "    for i in vals:\n",
    "        C= i\n",
    "        for j in vals:\n",
    "            gamma = 1/j\n",
    "            classifier = SVC(C=C,gamma=gamma)\n",
    "            classifier.fit(X,y)\n",
    "            prediction = classifier.predict(Xval)\n",
    "            score = classifier.score(Xval,yval)\n",
    "            if score>acc:\n",
    "                acc =score\n",
    "                best_c =C\n",
    "                best_gamma=gamma\n",
    "    return best_c, best_gamma\n",
    "\n",
    "C_vals = [0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n",
    "C, gamma = dataset3Params(X3, Y3.ravel(), Xval, Yval.ravel(),C_vals)\n",
    "classifier4 = SVC(C=C,gamma=gamma)\n",
    "print(classifier4)\n",
    "classifier4.fit(X3,Y3.ravel())\n",
    "print('Best values:\\nC =',C,'\\ngamma =',gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(CLR31[:,0],CLR31[:,1],\"go\")\n",
    "plt.plot(CLR32[:,0],CLR32[:,1],\"rx\")\n",
    "plt.title(\"Third data set and the decision boundary\")\n",
    "\n",
    "# plotting the decision boundary\n",
    "X_7,X_8 = np.meshgrid(np.linspace(X3[:,0].min(),X3[:,1].max(),num=100),np.linspace(X3[:,1].min(),X3[:,1].max(),num=100))\n",
    "desBoundary4 = classifier4.predict(np.array([X_7.ravel(),X_8.ravel()]).T).reshape(X_7.shape)\n",
    "plt.contour(X_7,X_8,desBoundary4,colors=\"b\")\n",
    "plt.xlim(-0.6,0.3)\n",
    "plt.ylim(-0.7,0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
