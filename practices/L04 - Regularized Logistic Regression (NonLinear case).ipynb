{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L04%20-%20Regularized%20Logistic%20Regression%20(NonLinear%20case).ipynb)\n",
    "\n",
    "# Labor 04: Regularized Logistic Regression (Non-Linear case)\n",
    "\n",
    "### Microchip Anomaly:\n",
    "\n",
    "Our task is to determine which of the microchips are defective and which are not, based on the measurement results.\n",
    "\n",
    "In this exercise we will work with a non-linearly separable data set. We want to use logistic regression for classification, so we extend the non-linear case by introducing more features (polynomial regression).\n",
    "\n",
    "In this exercise we will test different regularization parameters to better understand how regularization (penalty) works and how it can be used to prevent overfitting. Observe the changes in the decision boundary as the lamdba will be changed. With a small lambda you will notice that there is almost no error in the clustering, but in return you get a very complicated curve. This is not a good decision curve, notice that it accepts (-0.25; 1.5), which seems to be an incorrect decision based on our data set.\n",
    "\n",
    "Using a larger lambda we can see that a simpler decision boundary is created that does not follow the data as closely so it is underfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import neccessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be loaded from a publicly available file. An alternative solution would be to upload the data file directly to the google colab file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab04/Lab4data.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data! Use the Pandas package to do this and then convert it into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('Lab4data.txt', header = None).to_numpy()\n",
    "X = data[:,0:2]                                                     \n",
    "m,n = X.shape                                                       \n",
    "Y = data[:,2].reshape(m,1)                                         \n",
    "del data                                                           \n",
    "\n",
    "print('X:', X.shape)                    \n",
    "print('Y:', Y.shape)\n",
    "print('Number of data points:',m)\n",
    "print('Number of features:',n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the elements based on whether they passed the test or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotData(X,Y):\n",
    "    pos = []                                                        \n",
    "    neg = []                                                        \n",
    "\n",
    "    for i in range(0,Y.size):                                       \n",
    "        if Y[i]==1:                                                 \n",
    "            pos.append(X[i,:])\n",
    "        elif Y[i]==0:                                               \n",
    "            neg.append(X[i,:])\n",
    "\n",
    "    pos = np.array(pos)                                             \n",
    "    neg = np.array(neg)                                            \n",
    "\n",
    "    plt.scatter(pos[:, 0], pos[:, 1], c=\"g\", marker=\"o\", label=\"OK\")      \n",
    "    plt.scatter(neg[:, 0], neg[:, 1], c=\"r\", marker=\"x\", label=\"Not OK\")   \n",
    "    plt.title(\"Training data\")\n",
    "    plt.xlabel(\"Test 1\")\n",
    "    plt.ylabel(\"Test 2\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return pos,neg                                                  \n",
    "\n",
    "pos, neg = plotData(X,Y)                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our data are not linearly separable. <br>\n",
    "Since our data fall within the interval $[-1, 1]$ for both variables with a fairly good distribution, the data set does not require further normalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Theoretical background: Model Fit on Data\n",
    "\n",
    "In terms of model fit for the samples, we can distinguish 3 different cases.\n",
    "\n",
    "- Underfit (or High Bias) is when the model is too simple and therefore causes a large error on both the training data and the test data.\n",
    "\n",
    "- It is a good fit when we get low error on both the training data and the test data. This means that the learning process has succeeded in learning the relevant information that allows the new patterns to be classified well enough.\n",
    "\n",
    "- Overfit (or High Variance) if the model has learned the training patterns specifically during the training. This results in a very low error rate during learning, but a high error rate when classifying samples not included in the training data set. \n",
    "\n",
    "<img src=\"assets/Lab04/Pics/L04_Fittings.png\" width=\"800\">\n",
    "\n",
    "The Underfit and Overfit phenomena during a typical learning run. The goal would be to stop learning at the point where the validation error is the smallest. \n",
    "\n",
    "<img src=\"assets/Lab04/Pics/L04_BiasVariance.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we divide our available data into 3 groups if we have a large enough sample size.\n",
    "- Training set (~70%): data used during training to set the weights of the moddell\n",
    "- Validation set (~15%): Stop learning at the appropriate epoch, hyperparameter optimization\n",
    "- Test set (~15%): Test on independent data, define metrics\n",
    "\n",
    "In this example, we are working with a small data set, so we will not split the data in this way. Theoretical insight will help to understand.\n",
    "\n",
    "#### Methods to deal with Underfit and Overfit cases:\n",
    "\n",
    "For underfit, our model is too simple. Suitable compensation can be: <br>\n",
    "- Increase the number of features\n",
    "- Chose a more complex model\n",
    "\n",
    "In the case of overfit, our model learns the training data too specifically. Suitable compensation can be: <br>\n",
    "- Decrease the number of features\n",
    "- Chose a more simpler model\n",
    "- Regularization\n",
    "\n",
    "In the case of regularization, the guiding principle is: starting from a complex model (e.g.: multiple variables), the algorithm has many possibilities, thus the underfit phenomenon is most likely handled. And by extending the cost function, we penalize if the model uses too many variables. In this way, we create the optimal condition for the simplest model to solve the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Input feature expansion\n",
    "\n",
    "In the microchip test example the two test results are the input variables. To solve the desired problem more variables are needed. One possible way to do this is to increase the input variables by the powers of the original variables. \n",
    "\n",
    "$x_1,\\  x_2 \\Rightarrow\\ 1,\\ x_1,\\ x_2,\\ x_1^2,\\ x_1x_2,\\ x_2^2,\\ x_1^3,\\ x_1^2x_2,\\ x_1x_2^2,\\ x_2^3$\n",
    "\n",
    "By expanding the input variables to the 3rd power, including the BIAS term, we can count 10 variables instead of the initial 2.\n",
    "\n",
    "Let's create the mapFeature() function that performs the above mapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(X1,X2,deg):                            \n",
    "    ################### CODE HERE ######################## \n",
    "    # Implement the map feature function\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "          \n",
    "    ######################################################            \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg = 3\n",
    "X=mapFeature(X[:,0], X[:,1], deg)                       \n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the input variables have been expanded, we can move on to the construction of the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Cost function and Gradient Descent Method\n",
    "\n",
    "The activation function will be the sigmoid function, as we have used it in the previous labs.\n",
    "\n",
    "The cost function will be augmented with a penalty term, according to the formula below:\n",
    "\n",
    "$ C(w)=\\frac{1}{2m}\\sum_{i=1}^m(h_w(x^i)-y^i)^2+\\lambda\\sum_{\\color{red}{j=1}}^nw_{\\color{red}{j}}^2 $\n",
    "\n",
    ", where <br> \n",
    "$ \\lambda $ is the parameter setting the penalty rate <br>\n",
    "$ i $ is the index starting from 1. $i = 1...n$ <br>\n",
    "$ j $ is the index of the input variables starting from 0-tól indulva. $\\color{red}{ j = 0...m}$ <br>\n",
    "\n",
    "What to look out for to ensure that BIAS is not penalised. The weight $ \\color{red}{w_0}$ connected to the $x_{\\color{red}{0}} = 1 $ BIAS should not be taken into account when calculating the penalty. \n",
    "\n",
    "Setting the $ \\lambda $ value too high can lead to an underfit \n",
    "\n",
    "Let's consider how the cost function with the regularization term extended to the multivariate case and how it will fit into the gradient descent method.\n",
    "\n",
    "$ C(w)=-\\frac{1}{m}\\sum_{i=1}^{m}y^i\\cdot log(h_w(x^i))+(1-y^i)\\cdot log(1-h_w(x^i))+\\frac{\\lambda}{2m}\\sum_{j=1}^nw_j^2 $\n",
    "\n",
    "For ease of derivation, the constant in the regularizing term contains $\\frac{\\lambda}{2m}$ instead of $ \\lambda $ .\n",
    "\n",
    "Basic formula for the weight update of the Gradient Descent method:\n",
    "\n",
    "$ w_j = w_j - \\mu \\color{blue}{\\frac{\\partial}{\\partial w_j}C(w)}$\n",
    "\n",
    "When calculating the derivative of the cost function, the BIAS case should be treated separately. Let us examine the case of $w_0$ and $w_1$.\n",
    "\n",
    "$ \\color{blue}{\\frac{\\partial}{\\partial w_0}C(w)}=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_0^i+{\\color{red} 0}$\n",
    "\n",
    "$ \\color{blue}{\\frac{\\partial}{\\partial w_j}C(w)}=\\frac{1}{m}\\sum_{i=1}^{m}(h_w(x^i)-y^i)\\cdot x_j^i+\\frac{\\lambda}{m}w_j $\n",
    "\n",
    "Taking advantage of the similarities, consider how the cost function and the gradient could be calculated in a function using matrix operations. <br>\n",
    "Implement the costFunctionReg()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def costFunctionReg(w,X,Y,Lambda=1):\n",
    "    ################### CODE HERE ########################     \n",
    "    # Implement the cost functon with regularization\n",
    "    # First calculate the so called hipothesis. \n",
    "    # Remember that the BIAS is not penalized. Make sure to exclude the BIAS.\n",
    "    # Calculate the Logistic Regression Cost with the additional penalty.\n",
    "    # Calculate the gradiens as well.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "                                    \n",
    "    \n",
    "    ######################################################\n",
    "\n",
    "    return C,grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_w = np.zeros((X.shape[1],1))                   # init weights with zero\n",
    "C, grad =costFunctionReg(init_w,X,Y)                \n",
    "print('Expected cost at initial weight (zeros): 0.693')\n",
    "print('Calculated cost at initial weight (zeros): %.4f' % C[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the successful implementation of the cost function and the calculation of the gradients, the weights can be modified. <br>\n",
    "\n",
    "Basic weight modification formula for Gradient Descent algorithm:\n",
    "\n",
    "$w_j:=w_j-\\mu\\frac{\\partial}{\\partial w_j}C(w)$ \n",
    "\n",
    "Implement the gradientDescent() function. Use costFunctionReg() and save the cost of each epoch within the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,Y,w,learning_rate,num_iters,Lambda):                         \n",
    "    C_history = []\n",
    "\n",
    "    ################### CODE HERE ######################## \n",
    "    # Implement the gradient descent alorithm\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #####################################################\n",
    "        \n",
    "    return w, C_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us examine how the learning rate (or $\\mu$) and the regularization parameter ($\\lambda$) affect the evolution of the cost function. \n",
    "\n",
    "Experiment with different learning rate parameter settings to see what happens with the learning curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "epoch = 800\n",
    "Lambda = 0.02\n",
    "\n",
    "w, C_history = gradientDescent(X,Y,init_w,learning_rate,epoch,Lambda)      \n",
    "print('\\nRegularized weight:\\n',w)\n",
    "\n",
    "plt.plot(C_history,label = \"C_history\")    \n",
    "plt.title(\"Cost function trough the iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost function value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6: Visualization\n",
    "\n",
    "Plot the decision boundary on the original data series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pos[:, 0], pos[:, 1], c=\"g\", marker=\"o\", label=\"OK\")            \n",
    "plt.scatter(neg[:, 0], neg[:, 1], c=\"r\", marker=\"x\", label=\"Not OK\")        \n",
    "\n",
    "u_vals = np.linspace(-1,1.,50)                                              \n",
    "v_vals = np.linspace(-1,1.,50)                                              \n",
    "z=np.zeros((len(u_vals),len(v_vals)))                                       \n",
    "\n",
    "for i in range(len(u_vals)):                                                \n",
    "    for j in range(len(v_vals)):\n",
    "        z[i,j] = mapFeature(u_vals[i],v_vals[j],deg) @ w                        \n",
    "\n",
    "plt.contour(u_vals,v_vals,z.transpose(),0)                                  \n",
    "plt.title(\"Decision boundary and the training data\")\n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7: Accuracy of the prediction\n",
    "\n",
    "Run the prediction on multiple input instances and observ the overall accuracy by summing up the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificationPrediction(w,X):\n",
    "    pred = (sigmoid(X @ w) > 0.5)\n",
    "    return ((np.sum(pred==Y)/m)*100)\n",
    "\n",
    "acc=classificationPrediction(w,X)\n",
    "print('\\nAccuracy of the classification:',acc, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
