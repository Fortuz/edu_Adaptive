{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/logo.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made by **Balázs Nagy** and **Márk Domokos**\n",
    "\n",
    "[<img src=\"assets/open_button.png\">](https://colab.research.google.com/github/Fortuz/edu_Adaptive/blob/main/practices/L06%20-%20Neural%20Networks%20Basics_solved.ipynb)\n",
    "\n",
    "# Labor 06 - Neural Networks Basics\n",
    "\n",
    "## Handwritten numbers II.\n",
    "\n",
    "In the next exercise we will move on to the use of neural networks. As an example exercise, we will use the recognition of handwritten numbers (0-9), which we did in the previous lab (L05). In this notebook we will take a slightly different approach and introduce the neurla network concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0: Introduction of Neural Networks\n",
    "\n",
    "The labs so far have taught us the basics of learning algorithms. We have extended our methods to the multi-variable case and have also dealt with regression and classification problems. \n",
    "\n",
    "The model so far can be summarized as shown in the figure below:\n",
    "\n",
    "<img src=\"assets/Lab06/Pics/L06_Neuron_01.png\" width=\"450\">\n",
    "\n",
    ", where <br>\n",
    "$x_{n}$ indicates the inputs, $x_{0} = 1$ is the BIAS <br>\n",
    "$w_{n}$ indicates the weights <br>\n",
    "$s$ is the summed output, including the weighted inputs <br>\n",
    "$a$ is the activation function (for example: sigmoid) <br>\n",
    "$y$ indicate the output <br>\n",
    "\n",
    "On closer inspection, this diagram actually resembles a biological neuron. \n",
    "\n",
    "<img src=\"assets/Lab06/Pics/L06_Neuron_02.png\" width=\"450\">\n",
    "\n",
    "So let us introduce the above model as a neuron or perceptron model.\n",
    "\n",
    "The neurons can be arranged in a multilayer network. This network is called a neural network or MLP (Multi Layer Perceptron).\n",
    "\n",
    "#### Multi Layer Perceptron (MLP) structure and operation\n",
    "\n",
    "Consider the following architecture:\n",
    "\n",
    "<img src=\"assets/Lab06/Pics/L06_NeuralNet.png\" width=\"350\">\n",
    "\n",
    "The sketched case consists of an input layer, a hidden layer and an output layer. The input layer contains 2 neurons without BIAS, the hidden layer contains 3 neurons without BIAS and the output layer consists of 1 neuron. BIAS members can be added layer by layer in the model and are associated with an appropriate weight to the next layer's elements. \n",
    "\n",
    "We can speak of a Fully Connected Network if each neuron is connected to all neurons in the next layer. Assuming a sigmoid activation function without BIAS members, let us review the Forward Step of the prediction and the dimensions of the required matrices and vectors. Weights are assumed to be known.\n",
    "\n",
    "$ \\underset{1\\times 2}{\\mathrm{x}} \\times \\underset{2\\times 3}{\\mathrm{w^{(1)}}} = \\underset{1\\times 3}{\\mathrm{s^{(2)}}} $\n",
    "\n",
    "$ \\underset{1\\times 3}{\\mathrm{a^{(2)}}} = f(\\underset{1\\times 3}{\\mathrm{s^{(2)}}}) = sigmoid(\\underset{1\\times 3}{\\mathrm{s^{(2)}}}) $ \n",
    "\n",
    "$ \\underset{1\\times 1}{\\mathrm{s^{(3)}}} = \\underset{1\\times 3}{\\mathrm{a^{(2)}}} \\times \\underset{3\\times 1}{\\mathrm{w^{(2)}}} $\n",
    "\n",
    "$ \\underset{1\\times 1}{\\mathrm{\\hat{y}}} = f(\\underset{1\\times 1}{\\mathrm{s^{(3)}}}) = sigmoid(\\underset{1\\times 1}{\\mathrm{s^{(3)}}}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this works in practice through our example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data will be loaded from a publicly available file. An alternative solution would be to upload the data file directly to the google colab file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab06/Lab6data.mat\n",
    "!wget https://github.com/Fortuz/edu_Adaptive/raw/main/practices/assets/Lab06/Lab6weights.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data! Use the Pandas package to do this and then convert it into a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loadmat(\"Lab6data.mat\")                         \n",
    "X = data[\"X\"]\n",
    "Y = data [\"y\"]\n",
    "m = X.shape[0]                                          \n",
    "del data\n",
    "print('''Shape of the dataset in order X and Y:\n",
    "''',X.shape,'\\n',Y.shape,'\\n')\n",
    "\n",
    "data = loadmat(\"Lab6weights.mat\")                       # load in pretrained weights\n",
    "w1 = data[\"Theta1\"]\n",
    "w2 = data[\"Theta2\"]                                     # arrange weights\n",
    "del data\n",
    "print('''Shape of the weights in order 1 and 2:\n",
    "''',w1.shape,'\\n',w2.shape,'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After examining the dimensions of the scanned data, we see that there are 5000 input samples. We have 400 input parameters, which corresponds to the number of pixels in 20x20 images. Our first weight matrix contains 401 columns, indicating that the input data has been augmented with the BIAS. The hidden layer contains 25 neurons, as indicated by the first dimension of the first weight matrix. The dimensions of the second weight matrix show that there are 10 output neurons in the model corresponding to the 10 digits and 26 bemental values when added with BIAS.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3: Visualization\n",
    "\n",
    "To visualise the data, we randomly select 100 samples, reshape them back to their original shape and plot them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Printing some random data ...\")\n",
    "fig, ax = plt.subplots(10,10, figsize =(8,8))               \n",
    "for i in range(10):\n",
    "    for j in range(10):                                     \n",
    "        ax[i,j].imshow(X[np.random.randint(0,m+1),:].reshape(20,20, order = \"F\"), cmap=\"hot\")\n",
    "        ax[i,j].axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Prediction and Accuracy\n",
    "\n",
    "Based on the theoretical introduction, we implement the prediction step (Forward Step). Pay attention to the addition of BIAS members and appropriate matrix dimensions. Since we are talking about a classification problem, we have to select the appropriate class of the calculated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):                                                  \n",
    "    return 1/(1+np.exp(-z))\n",
    "\n",
    "def predict(X,w1,w2):                                            \n",
    "    ################### CODE HERE ########################\n",
    "    # Implement the prediction (forward step) for a 2 layer Neural Network\n",
    "    # Given the input and the 2 weight matrix.\n",
    "    # Make sure the dimensions are matching.\n",
    "    # Add a Bias to each layer. \n",
    "    # The prediction can be calculated with a max pooling step at the end.\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "    #####################################################\n",
    "    \n",
    "    return pred                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the prediction function we have written. We randomly select an element and run the predict() function on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "i= random.randint(1,m+1)\n",
    "\n",
    "X_ex = X[i,:].reshape(1,-1)       # correct matrix dimensions and format due to reshape()\n",
    "pred_ex = predict(X_ex,w1,w2)\n",
    "\n",
    "print('Showing You a %.0f'% Y[i], '\\nThe prediction was %.0f' % pred_ex)\n",
    "fig = plt.figure(figsize=(2,2))\n",
    "plt.imshow(X[i,:].reshape(20,20).T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By performing the prediction on all the elements and comparing it with the original labels, we can see how well the algorithm learned the sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(pred,Y):                                            \n",
    "    return (np.sum(pred[:,np.newaxis]==Y)/5000)*100              \n",
    "\n",
    "\n",
    "pred = predict(X,w1,w2)\n",
    "print('\\nTraining set Accuracy: ', accuracy(pred,Y), ' %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">This lab exercise uses elements from Andrew Ng's Machine Learning course.</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
